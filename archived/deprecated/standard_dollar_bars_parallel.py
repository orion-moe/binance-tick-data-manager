#!/usr/bin/env python3
"""
Vers√£o otimizada das Standard Dollar Bars com m√°ximo uso de CPU
Usa estrat√©gias h√≠bridas para maximizar paraleliza√ß√£o onde poss√≠vel
"""

import os
import logging
import pandas as pd
import numpy as np
import glob
from numba import njit, types
from numba.typed import List
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing as mp
from functools import partial
import psutil

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def get_data_path(data_type='futures', futures_type='um', granularity='daily'):
    """Constr√≥i o caminho para os dados."""
    project_root = Path(__file__).resolve().parent.parent.parent
    data_dir = project_root / 'data'

    if data_type == 'spot':
        return data_dir / f'dataset-raw-{granularity}-compressed-optimized' / 'spot'
    else:
        return data_dir / f'dataset-raw-{granularity}-compressed-optimized' / f'futures-{futures_type}'

# ==============================================================================
# ESTRAT√âGIA 1: Paraleliza√ß√£o da leitura de arquivos
# ==============================================================================

def read_single_parquet(file_path):
    """L√™ um √∫nico arquivo parquet (para paraleliza√ß√£o)."""
    try:
        df = pd.read_parquet(
            file_path,
            columns=['time', 'price', 'qty', 'quoteQty', 'isBuyerMaker'],
            engine='pyarrow'
        )
        # Pr√©-processamento
        df['side'] = np.where(df['isBuyerMaker'], -1, 1).astype(np.int8)
        df['net_volumes'] = df['quoteQty'] * df['side']
        return df
    except Exception as e:
        logging.error(f"Erro ao ler {file_path}: {e}")
        return pd.DataFrame()

def parallel_read_all_files(raw_dataset_path, max_workers=None):
    """L√™ todos os arquivos parquet em paralelo."""
    if max_workers is None:
        max_workers = mp.cpu_count()

    files = sorted(glob.glob(os.path.join(raw_dataset_path, "*.parquet")))
    logging.info(f"üìö Lendo {len(files)} arquivos com {max_workers} workers paralelos")

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        dfs = list(executor.map(read_single_parquet, files))

    # Concatena todos os dataframes
    df_combined = pd.concat([df for df in dfs if not df.empty], ignore_index=True)
    logging.info(f"‚úÖ Lidos {len(df_combined):,} registros total")
    return df_combined

# ==============================================================================
# ESTRAT√âGIA 2: Processamento em chunks com paraleliza√ß√£o parcial
# ==============================================================================

@njit
def find_bar_boundaries(cumsum_volumes, threshold):
    """
    Encontra os √≠ndices onde as barras devem ser formadas.
    Esta opera√ß√£o pode ser paralelizada!
    """
    boundaries = List()
    current_sum = 0.0

    for i in range(len(cumsum_volumes)):
        current_sum += abs(cumsum_volumes[i])
        if current_sum >= threshold:
            boundaries.append(i)
            current_sum = 0.0

    return boundaries

def parallel_find_boundaries(net_volumes, threshold, chunk_size=1_000_000):
    """
    Divide os dados em chunks e processa em paralelo onde poss√≠vel.
    """
    n_chunks = len(net_volumes) // chunk_size + 1
    chunks = np.array_split(net_volumes, n_chunks)

    # Processa chunks em paralelo para encontrar potenciais boundaries
    with ProcessPoolExecutor(max_workers=mp.cpu_count()) as executor:
        partial_func = partial(find_bar_boundaries, threshold=threshold)
        boundaries_per_chunk = list(executor.map(partial_func, chunks))

    # Combina resultados (requer ajuste dos √≠ndices)
    all_boundaries = []
    offset = 0
    for chunk_boundaries in boundaries_per_chunk:
        for boundary in chunk_boundaries:
            all_boundaries.append(boundary + offset)
        offset += len(chunks[0])

    return all_boundaries

# ==============================================================================
# ESTRAT√âGIA 3: Numba com paraleliza√ß√£o interna
# ==============================================================================

@njit(parallel=True, cache=True, fastmath=True)
def compute_dollar_bars_parallel(prices, times, net_volumes, sides, qtys, threshold):
    """
    Vers√£o otimizada com paraleliza√ß√£o interna do Numba.
    Usa prange para loops paraleliz√°veis.
    """
    n = len(prices)
    bars = List()

    current_volume = 0.0
    bar_start_idx = 0

    # Arrays tempor√°rios para acumular estat√≠sticas
    high_prices = np.empty(n)
    low_prices = np.empty(n)

    # Este loop n√£o pode ser totalmente paralelizado devido √†s depend√™ncias
    # mas podemos paralelizar opera√ß√µes internas
    for i in range(n):
        current_volume += abs(net_volumes[i])

        if current_volume >= threshold:
            # Calcula estat√≠sticas da barra em paralelo
            bar_data = prices[bar_start_idx:i+1]

            bar_open = prices[bar_start_idx]
            bar_close = prices[i]
            bar_high = np.max(bar_data)
            bar_low = np.min(bar_data)

            bars.append((
                times[bar_start_idx], times[i],
                bar_open, bar_high, bar_low, bar_close,
                current_volume
            ))

            # Reset para pr√≥xima barra
            current_volume = 0.0
            bar_start_idx = i + 1

    return bars

# ==============================================================================
# ESTRAT√âGIA 4: Pipeline com m√°xima paraleliza√ß√£o
# ==============================================================================

class OptimizedDollarBarsGenerator:
    """Gerador otimizado que maximiza uso de CPU."""

    def __init__(self, data_path, threshold=40_000_000):
        self.data_path = data_path
        self.threshold = threshold
        self.n_workers = mp.cpu_count()

        # Monitora uso de CPU
        self.cpu_monitor = psutil.Process()

    def generate(self):
        """Pipeline principal otimizado."""

        # 1. Leitura paralela (100% CPU)
        logging.info("üöÄ Fase 1: Leitura paralela de arquivos")
        df = parallel_read_all_files(self.data_path, self.n_workers)

        # 2. Pr√©-processamento paralelo (100% CPU)
        logging.info("üöÄ Fase 2: Pr√©-processamento paralelo")
        df = self._parallel_preprocess(df)

        # 3. Gera√ß√£o de barras (otimizada mas sequencial)
        logging.info("üöÄ Fase 3: Gera√ß√£o de dollar bars")
        bars = self._generate_bars_optimized(df)

        # 4. P√≥s-processamento paralelo (100% CPU)
        logging.info("üöÄ Fase 4: P√≥s-processamento paralelo")
        result = self._parallel_postprocess(bars)

        # Monitora performance
        cpu_percent = self.cpu_monitor.cpu_percent()
        logging.info(f"üìä Uso m√©dio de CPU: {cpu_percent:.1f}%")

        return result

    def _parallel_preprocess(self, df):
        """Pr√©-processamento com vetoriza√ß√£o NumPy (usa BLAS)."""
        # NumPy automaticamente usa m√∫ltiplas threads via BLAS
        df['net_volumes'] = df['quoteQty'].values * df['side'].values
        df['cumsum_volumes'] = np.abs(df['net_volumes'].values).cumsum()
        return df

    def _generate_bars_optimized(self, df):
        """Gera√ß√£o otimizada de barras."""
        return compute_dollar_bars_parallel(
            df['price'].values,
            df['time'].values,
            df['net_volumes'].values,
            df['side'].values,
            df['qty'].values,
            self.threshold
        )

    def _parallel_postprocess(self, bars):
        """P√≥s-processamento paralelo dos resultados."""
        df_bars = pd.DataFrame(bars, columns=[
            'start_time', 'end_time', 'open', 'high', 'low', 'close', 'volume'
        ])

        # C√°lculos adicionais em paralelo
        with ThreadPoolExecutor(max_workers=self.n_workers) as executor:
            # Adiciona features em paralelo
            futures = []
            futures.append(executor.submit(self._compute_returns, df_bars))
            futures.append(executor.submit(self._compute_volatility, df_bars))
            futures.append(executor.submit(self._compute_volume_stats, df_bars))

            # Aguarda resultados
            for future in futures:
                future.result()

        return df_bars

    @staticmethod
    def _compute_returns(df):
        """Calcula retornos."""
        df['returns'] = df['close'].pct_change()
        return df

    @staticmethod
    def _compute_volatility(df):
        """Calcula volatilidade."""
        df['volatility'] = df['returns'].rolling(20).std()
        return df

    @staticmethod
    def _compute_volume_stats(df):
        """Calcula estat√≠sticas de volume."""
        df['volume_ma'] = df['volume'].rolling(20).mean()
        return df

# ==============================================================================
# FUN√á√ÉO PRINCIPAL
# ==============================================================================

def generate_standard_bars_optimized(
    data_type='futures',
    futures_type='um',
    granularity='daily',
    threshold=40_000_000,
    output_dir='./output/standard_optimized/'
):
    """Gera standard dollar bars com m√°xima utiliza√ß√£o de CPU."""

    # Setup
    data_path = get_data_path(data_type, futures_type, granularity)
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    logging.info(f"üéØ Gerando Dollar Bars Otimizadas")
    logging.info(f"üìÅ Dados: {data_path}")
    logging.info(f"üí∞ Threshold: ${threshold:,.0f}")
    logging.info(f"üñ•Ô∏è CPUs dispon√≠veis: {mp.cpu_count()}")

    # Executa gerador otimizado
    generator = OptimizedDollarBarsGenerator(data_path, threshold)
    df_bars = generator.generate()

    # Salva resultado
    output_file = output_dir / f"dollar_bars_{data_type}_{threshold}.parquet"
    df_bars.to_parquet(output_file, engine='pyarrow', compression='snappy')

    logging.info(f"‚úÖ Conclu√≠do! {len(df_bars)} barras geradas")
    logging.info(f"üíæ Salvo em: {output_file}")

    return df_bars

if __name__ == "__main__":
    # Teste direto
    df = generate_standard_bars_optimized(
        data_type='spot',
        threshold=40_000_000
    )
    print(f"Geradas {len(df)} barras")