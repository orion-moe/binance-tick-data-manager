{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 58761 instead\n",
      "  warnings.warn(\n",
      "INFO:root:<Client: 'tcp://127.0.0.1:58762' processes=10 threads=10, memory=59.60 GiB>\n",
      "INFO:root:Dask processando arquivo 1 de 12\n",
      "INFO:root:Processando partição 1 de 43\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 229\u001b[0m\n\u001b[1;32m    227\u001b[0m     df_dask \u001b[38;5;241m=\u001b[39m read_parquet_files_optimized(raw_dataset_path, file)\n\u001b[1;32m    228\u001b[0m     df_dask \u001b[38;5;241m=\u001b[39m apply_operations_optimized(df_dask, meta)\n\u001b[0;32m--> 229\u001b[0m     bars, init_T, init_dif, res \u001b[38;5;241m=\u001b[39m batch_create_imbalance_dollar_bars_optimized(\n\u001b[1;32m    230\u001b[0m         df_dask, init_T, init_dif, res, alpha_volume, alpha_imbalance\n\u001b[1;32m    231\u001b[0m     )\n\u001b[1;32m    232\u001b[0m     results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([results, bars], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    234\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha_volume\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha_imbalance\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minit_T\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[2], line 168\u001b[0m, in \u001b[0;36mbatch_create_imbalance_dollar_bars_optimized\u001b[0;34m(df_dask, init_T, init_dif, res_init, alpha_volume, alpha_imbalance)\u001b[0m\n\u001b[1;32m    165\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessando partição \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpartition\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m de \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_dask\u001b[38;5;241m.\u001b[39mnpartitions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    167\u001b[0m part \u001b[38;5;241m=\u001b[39m df_dask\u001b[38;5;241m.\u001b[39mget_partition(partition)\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m--> 168\u001b[0m bars, init_T, init_dif, res_init \u001b[38;5;241m=\u001b[39m create_imbalance_dollar_bars(\n\u001b[1;32m    169\u001b[0m     part, init_T, init_dif, res_init, alpha_volume, alpha_imbalance\n\u001b[1;32m    170\u001b[0m )\n\u001b[1;32m    171\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(pd\u001b[38;5;241m.\u001b[39mDataFrame(bars))\n\u001b[1;32m    173\u001b[0m part_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Fim do tempo de execução para o batch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 84\u001b[0m, in \u001b[0;36mcreate_imbalance_dollar_bars\u001b[0;34m(partition, init_T, init_dif, res_init, alpha_volume, alpha_imbalance)\u001b[0m\n\u001b[1;32m     81\u001b[0m volume_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqty\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m partition\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m bar_open \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m             bar_open \u001b[38;5;241m=\u001b[39m row[price_col]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:1554\u001b[0m, in \u001b[0;36mDataFrame.iterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1552\u001b[0m using_cow \u001b[38;5;241m=\u001b[39m using_copy_on_write()\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[0;32m-> 1554\u001b[0m     s \u001b[38;5;241m=\u001b[39m klass(v, index\u001b[38;5;241m=\u001b[39mcolumns, name\u001b[38;5;241m=\u001b[39mk)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m using_cow \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block:\n\u001b[1;32m   1556\u001b[0m         s\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39madd_references(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:584\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    582\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m     data \u001b[38;5;241m=\u001b[39m sanitize_array(data, index, dtype, copy)\n\u001b[1;32m    586\u001b[0m     manager \u001b[38;5;241m=\u001b[39m _get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/construction.py:606\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    604\u001b[0m subarr \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m--> 606\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m maybe_infer_to_datetimelike(data)\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    608\u001b[0m         object_index\n\u001b[1;32m    609\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m using_pyarrow_string_dtype()\n\u001b[1;32m    610\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m is_string_dtype(subarr)\n\u001b[1;32m    611\u001b[0m     ):\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;66;03m# Avoid inference when string option is set\u001b[39;00m\n\u001b[1;32m    613\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/dtypes/cast.py:1189\u001b[0m, in \u001b[0;36mmaybe_infer_to_datetimelike\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# error: Incompatible return value type (got \"Union[ExtensionArray,\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# ndarray[Any, Any]]\", expected \"Union[ndarray[Any, Any], DatetimeArray,\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# TimedeltaArray, PeriodArray, IntervalArray]\")\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(  \u001b[38;5;66;03m# type: ignore[return-value]\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m     value,\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;66;03m# Here we do not convert numeric dtypes, as if we wanted that,\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;66;03m#  numpy would have done it for us.\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m     convert_numeric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1194\u001b[0m     convert_non_numeric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1195\u001b[0m     dtype_if_all_nat\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM8[ns]\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1196\u001b[0m )\n",
      "File \u001b[0;32mlib.pyx:2543\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/numpy/core/numeric.py:330\u001b[0m, in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype, order, like)\u001b[0m\n\u001b[1;32m    328\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m fill_value\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    329\u001b[0m a \u001b[38;5;241m=\u001b[39m empty(shape, dtype, order)\n\u001b[0;32m--> 330\u001b[0m multiarray\u001b[38;5;241m.\u001b[39mcopyto(a, fill_value, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/numpy/core/multiarray.py:1080\u001b[0m, in \u001b[0;36mcopyto\u001b[0;34m(dst, src, casting, where)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;124;03m    unravel_index(indices, shape, order='C')\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1075\u001b[0m \n\u001b[1;32m   1076\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (indices,)\n\u001b[0;32m-> 1080\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mcopyto)\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopyto\u001b[39m(dst, src, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;124;03m    copyto(dst, src, casting='same_kind', where=True)\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \n\u001b[1;32m   1127\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (dst, src, where)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dask.dataframe as dd\n",
    "import datetime\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time  # Para medir o tempo de execução\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Configurar o logging para visualizar os logs de informação\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Configurar o cliente Dask\n",
    "client = Client(n_workers=10, threads_per_worker=1, memory_limit='6.4GB')\n",
    "logging.info(client)\n",
    "\n",
    "raw_dataset_path = '../datasets/BTCUSDT-Trades-compressed/'\n",
    "output_base_path = '../output'\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "output_path = f'{output_base_path}_v{timestamp}'\n",
    "\n",
    "def read_parquet_files_optimized(raw_dataset_path, file):\n",
    "    parquet_pattern = os.path.join(raw_dataset_path, file)\n",
    "    df_dask = dd.read_parquet(\n",
    "        parquet_pattern,\n",
    "        columns=['price', 'qty', 'quoteQty', 'time'],\n",
    "        engine='pyarrow',\n",
    "        dtype={'price': 'float32', 'qty': 'float32', 'quoteQty': 'float32'}\n",
    "    )\n",
    "    return df_dask\n",
    "\n",
    "def assign_side_optimized(df):\n",
    "    df['side'] = np.where(df['price'].shift() > df['price'], 1,\n",
    "                          np.where(df['price'].shift() < df['price'], -1, np.nan))\n",
    "    # Preencher NaNs após ffill() com um valor padrão, por exemplo, 1\n",
    "    df['side'] = df['side'].ffill().fillna(1).astype('int8')\n",
    "    return df\n",
    "\n",
    "def apply_operations_optimized(df_dask, meta):\n",
    "    df_dask = df_dask.map_partitions(assign_side_optimized, meta=meta)\n",
    "    df_dask['dollar_imbalance'] = df_dask['quoteQty'] * df_dask['side']\n",
    "    return df_dask\n",
    "\n",
    "def create_imbalance_dollar_bars(partition, init_T, init_dif, res_init, alpha_volume, alpha_imbalance):\n",
    "    \"\"\"\n",
    "    Função original para criar barras de desequilíbrio em dólares.\n",
    "    \"\"\"\n",
    "    exp_T     = init_T\n",
    "    exp_dif   = init_dif\n",
    "    threshold = exp_T * init_dif\n",
    "\n",
    "    bars = []\n",
    "\n",
    "    # Variáveis de agregação de uma barra\n",
    "    if len(res_init) > 0:\n",
    "        bar_open = res_init[0]\n",
    "        bar_high = res_init[1]\n",
    "        bar_low = res_init[2]\n",
    "        bar_close = res_init[3]\n",
    "        bar_start_time = res_init[4]\n",
    "        bar_end_time = res_init[5]\n",
    "        current_imbalance = res_init[6]\n",
    "        buy_volume_usd = res_init[7]\n",
    "        total_volume_usd = res_init[8]\n",
    "        total_volume = res_init[9]\n",
    "    else:\n",
    "        bar_open = None\n",
    "        bar_high = -float('inf')\n",
    "        bar_low = float('inf')\n",
    "        bar_close = None\n",
    "        bar_start_time = None\n",
    "        bar_end_time = None\n",
    "        current_imbalance = 0\n",
    "        buy_volume_usd = 0\n",
    "        total_volume_usd = 0\n",
    "        total_volume = 0\n",
    "\n",
    "    price_col = 'price'\n",
    "    time_col = 'time'\n",
    "    imbalance_col = 'dollar_imbalance'\n",
    "    volume_col = 'qty'\n",
    "\n",
    "    try:\n",
    "        for idx, row in partition.iterrows():\n",
    "            if bar_open is None:\n",
    "                bar_open = row[price_col]\n",
    "                bar_start_time = row[time_col]\n",
    "\n",
    "            # Atualiza valores de OHLC\n",
    "            trade_price = row[price_col]\n",
    "            bar_high    = max(bar_high, trade_price)\n",
    "            bar_low     = min(bar_low, trade_price)\n",
    "            bar_close   = trade_price\n",
    "\n",
    "            # Soma o volume (ou outra métrica de desequilíbrio)\n",
    "            trade_imbalance = row[imbalance_col]\n",
    "\n",
    "            if row['side'] > 0:\n",
    "                buy_volume_usd += trade_imbalance\n",
    "\n",
    "            total_volume += row[volume_col]\n",
    "            total_volume_usd += abs(trade_imbalance)\n",
    "            current_imbalance += trade_imbalance\n",
    "            imbalance = abs(current_imbalance)\n",
    "            # Verifica se a soma já ultrapassou o threshold\n",
    "            if imbalance >= threshold:\n",
    "                bar_end_time = row[time_col]\n",
    "\n",
    "                # Salvar a barra formada\n",
    "                bars.append({\n",
    "                    'start_time': bar_start_time,\n",
    "                    'end_time': bar_end_time,\n",
    "                    'open': bar_open,\n",
    "                    'high': bar_high,\n",
    "                    'low': bar_low,\n",
    "                    'close': bar_close,\n",
    "                    'imbalance_col': current_imbalance,\n",
    "                    'total_volume_buy_usd': buy_volume_usd,\n",
    "                    'total_volume_usd': total_volume_usd,\n",
    "                    'total_volume': total_volume\n",
    "                })\n",
    "\n",
    "                # Exponential-weighted updates\n",
    "                if exp_dif == 1:\n",
    "                    exp_T   = total_volume_usd\n",
    "                    exp_dif = abs(2 * buy_volume_usd / total_volume_usd - 1)\n",
    "                else:\n",
    "                    exp_T   += alpha_volume * (total_volume_usd   - exp_T)\n",
    "                    exp_dif += alpha_imbalance * (abs(2 * buy_volume_usd / total_volume_usd - 1) - exp_dif)\n",
    "                # Reset accumulators\n",
    "                threshold = exp_T * abs(exp_dif)\n",
    "\n",
    "                # Variáveis de agregação de uma barra\n",
    "                bar_open = None\n",
    "                bar_high = -float('inf')\n",
    "                bar_low = float('inf')\n",
    "                bar_close = None\n",
    "                bar_start_time = None\n",
    "                bar_end_time = None\n",
    "                current_imbalance = 0\n",
    "                buy_volume_usd = 0\n",
    "                total_volume_usd = 0\n",
    "                total_volume = 0\n",
    "    finally:\n",
    "        if current_imbalance == 0:\n",
    "            res_init = []\n",
    "        else:\n",
    "            res_init = [bar_open, bar_high, bar_low, bar_close, bar_start_time,\n",
    "                       bar_end_time, current_imbalance, buy_volume_usd, total_volume_usd, total_volume]\n",
    "\n",
    "    return bars, exp_T, exp_dif, res_init\n",
    "\n",
    "def batch_create_imbalance_dollar_bars_optimized(df_dask, init_T, init_dif, res_init, alpha_volume, alpha_imbalance):\n",
    "    results = []\n",
    "    start_time = time.time()  # Inicia a contagem do tempo total\n",
    "\n",
    "    # Variáveis para cálculo de tempo estimado\n",
    "    total_time_spent = 0  # Tempo total gasto até o momento\n",
    "    completed_batches = 0  # Contador de batches processados\n",
    "    estimated_time_left = 0  # Estimativa de tempo restante\n",
    "\n",
    "    for partition in range(df_dask.npartitions):\n",
    "        part_start_time = time.time()  # Inicia o tempo de execução para o batch\n",
    "\n",
    "        logging.info(f\"Processando partição {partition + 1} de {df_dask.npartitions}\")\n",
    "\n",
    "        part = df_dask.get_partition(partition).compute()\n",
    "        bars, init_T, init_dif, res_init = create_imbalance_dollar_bars(\n",
    "            part, init_T, init_dif, res_init, alpha_volume, alpha_imbalance\n",
    "        )\n",
    "        results.append(pd.DataFrame(bars))\n",
    "\n",
    "        part_end_time = time.time()  # Fim do tempo de execução para o batch\n",
    "        batch_time = part_end_time - part_start_time\n",
    "        total_time_spent += batch_time\n",
    "        completed_batches += 1\n",
    "\n",
    "        # Calcula o tempo médio por batch\n",
    "        average_time_per_batch = total_time_spent / completed_batches\n",
    "\n",
    "        # Calcula o número de batches restantes\n",
    "        remaining_batches = df_dask.npartitions - completed_batches\n",
    "\n",
    "        # Estima o tempo restante\n",
    "        estimated_time_left = average_time_per_batch * remaining_batches\n",
    "\n",
    "        # Exibe informações de progresso\n",
    "        logging.info(f\"Tempo médio por batch: {average_time_per_batch:.2f} segundos\")\n",
    "        logging.info(f\"Batches restantes: {remaining_batches}\")\n",
    "        logging.info(f\"Tempo estimado restante: {estimated_time_left / 60:.2f} minutos\")  # Tempo restante em minutos\n",
    "\n",
    "    results_df = pd.concat(results, ignore_index=True)\n",
    "    end_time = time.time()  # Fim do tempo total\n",
    "    logging.info(f\"Processamento completo em {total_time_spent / 60:.2f} minutos.\")\n",
    "    return results_df, init_T, init_dif, res_init\n",
    "\n",
    "# Definir o meta DataFrame para map_partitions\n",
    "meta = pd.DataFrame({\n",
    "    'price': pd.Series(dtype='float32'),\n",
    "    'qty': pd.Series(dtype='float32'),\n",
    "    'quoteQty': pd.Series(dtype='float32'),\n",
    "    'time': pd.Series(dtype='datetime64[ns]'),\n",
    "    'side': pd.Series(dtype='int8')  # Adicionando a coluna 'side' como int8\n",
    "})\n",
    "\n",
    "# Listar arquivos\n",
    "files = [f for f in os.listdir(raw_dataset_path) if os.path.isfile(os.path.join(raw_dataset_path, f))]\n",
    "file_count = len(files)\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "init_T = 10_000\n",
    "init_dif = 1\n",
    "alpha_volume = 0.1\n",
    "alpha_imbalance = 0.9\n",
    "res = []\n",
    "\n",
    "for number in range(1, file_count + 1):  # Ajuste para incluir o último arquivo\n",
    "    logging.info(f\"Dask processando arquivo {number} de {file_count}\")\n",
    "    file = f'combined_file_{number}.parquet'\n",
    "\n",
    "    # Verifique se o arquivo existe para evitar erros\n",
    "    if not os.path.exists(os.path.join(raw_dataset_path, file)):\n",
    "        logging.warning(f\"Arquivo {file} não encontrado. Pulando para o próximo.\")\n",
    "        continue\n",
    "\n",
    "    df_dask = read_parquet_files_optimized(raw_dataset_path, file)\n",
    "    df_dask = apply_operations_optimized(df_dask, meta)\n",
    "    bars, init_T, init_dif, res = batch_create_imbalance_dollar_bars_optimized(\n",
    "        df_dask, init_T, init_dif, res, alpha_volume, alpha_imbalance\n",
    "    )\n",
    "    results = pd.concat([results, bars], ignore_index=True)\n",
    "\n",
    "output_file = f'{output_path}-{alpha_volume}-{alpha_imbalance}-{init_T}.xlsx'\n",
    "results.to_excel(output_file, index=False)\n",
    "logging.info(f\"Resultados salvos em {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
