{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import os\nos.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\nimport dask.dataframe as dd\nimport datetime\nimport logging\nimport numpy as np\nimport pandas as pd\nfrom dask.distributed import Client, progress\nfrom numba import njit, types\nfrom numba.typed import List\nimport time\nfrom sqlalchemy import create_engine\n\n# Configuração do logging\nlogging.basicConfig(level=logging.INFO)\n\n# Configuração do cliente Dask\nclient = Client(n_workers=10, threads_per_worker=1, memory_limit='6.4GB')\nlogging.info(client)\n\n# Caminhos dos arquivos - ajustados para o caminho correto\n# Detecta o caminho base do projeto\nnotebook_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\nproject_root = os.path.dirname(os.path.dirname(notebook_dir))\n\n# Configuração de parâmetros\nDATA_TYPE = 'futures'  # 'spot' ou 'futures'\nFUTURES_TYPE = 'um'    # 'um' ou 'cm' (apenas para futures)\nGRANULARITY = 'daily'  # 'daily' ou 'monthly'\n\n# Constrói o caminho correto baseado nos parâmetros\nif DATA_TYPE == 'spot':\n    raw_dataset_path = os.path.join(project_root, 'datasets', f'dataset-raw-{GRANULARITY}-compressed-optimized', 'spot')\nelse:\n    raw_dataset_path = os.path.join(project_root, 'datasets', f'dataset-raw-{GRANULARITY}-compressed-optimized', f'futures-{FUTURES_TYPE}')\n\noutput_base_path = os.path.join(project_root, 'output')\ntimestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\noutput_path = f'{output_base_path}_v{timestamp}'\n\n# Cria diretório de output se não existir\nos.makedirs(output_base_path, exist_ok=True)\n\nprint(f\"Raw dataset path: {raw_dataset_path}\")\nprint(f\"Output path: {output_base_path}\")\n\n# Verifica se o diretório existe\nif not os.path.exists(raw_dataset_path):\n    raise FileNotFoundError(f\"O diretório de dados não foi encontrado: {raw_dataset_path}\\n\"\n                          f\"Certifique-se de que você executou as etapas anteriores do pipeline.\")"
  },
  {
   "cell_type": "markdown",
   "source": "# Imbalance Dollar Bars v3\n\n## Descrição\nEste notebook gera \"imbalance dollar bars\" a partir de dados de trades do Bitcoin. Ele usa processamento distribuído com Dask e otimizações com Numba para processar grandes volumes de dados eficientemente.\n\n## Configuração\nAntes de executar este notebook, certifique-se de:\n\n1. **Executar o pipeline principal** (`main.py`) e completar as seguintes etapas:\n   - Step 1: Download dos dados\n   - Step 2: ZIP → CSV → Parquet Pipeline\n   - Step 3: Optimize Parquet files\n\n2. **Verificar os parâmetros** na primeira célula:\n   - `DATA_TYPE`: 'spot' ou 'futures'\n   - `FUTURES_TYPE`: 'um' ou 'cm' (apenas para futures)\n   - `GRANULARITY`: 'daily' ou 'monthly'\n\n3. **Dependências necessárias**:\n   - dask\n   - numba\n   - pandas\n   - numpy\n   - pyarrow\n   - openpyxl (para salvar Excel)\n\n## Output\nOs resultados são salvos como arquivos Excel no diretório `output/` na raiz do projeto.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def read_parquet_files_optimized(raw_dataset_path, file):\n    \"\"\"Lê arquivos Parquet de forma otimizada.\"\"\"\n    parquet_pattern = os.path.join(raw_dataset_path, file)\n    df_dask = dd.read_parquet(\n        parquet_pattern,\n        columns=['price', 'qty', 'quoteQty', 'time'],\n        engine='pyarrow',\n        dtype={'price': 'float32', 'qty': 'float32', 'quoteQty': 'float32'}\n    )\n    return df_dask\n\ndef assign_side_optimized(df):\n    \"\"\"Atribui o lado da negociação com base na mudança de preço.\"\"\"\n    df['side'] = np.where(df['price'].shift() > df['price'], 1,\n                          np.where(df['price'].shift() < df['price'], -1, np.nan))\n    df['side'] = df['side'].ffill().fillna(1).astype('int8')\n    return df\n\ndef apply_operations_optimized(df_dask, meta):\n    \"\"\"Aplica operações otimizadas no DataFrame.\"\"\"\n    df_dask = df_dask.map_partitions(assign_side_optimized, meta=meta)\n    df_dask['dollar_imbalance'] = df_dask['quoteQty'] * df_dask['side']\n    return df_dask\n\n# Função compilada com numba\n@njit(\n    types.Tuple((\n        types.ListType(types.Tuple((\n            types.float64,  # start_time\n            types.float64,  # end_time\n            types.float64,  # open\n            types.float64,  # high\n            types.float64,  # low\n            types.float64,  # close\n            types.float64,  # imbalance_col\n            types.float64,  # total_volume_buy_usd\n            types.float64,  # total_volume_usd\n            types.float64   # total_volume\n        ))),\n        types.float64,  # exp_T\n        types.float64,  # exp_dif\n        types.Tuple((\n            types.float64,  # bar_open\n            types.float64,  # bar_high\n            types.float64,  # bar_low\n            types.float64,  # bar_close\n            types.float64,  # bar_start_time\n            types.float64,  # bar_end_time\n            types.float64,  # current_imbalance\n            types.float64,  # buy_volume_usd\n            types.float64,  # total_volume_usd\n            types.float64   # total_volume\n        )),\n        types.ListType(types.Tuple((\n            types.float64,  # exp_T\n            types.float64,  # exp_dif\n            types.float64   # thres\n        ))),\n        types.int64     # warm_up_count\n    ))(\n        types.float64[:],  # prices\n        types.float64[:],  # times\n        types.float64[:],  # imbalances\n        types.int8[:],     # sides\n        types.float64[:],  # qtys\n        types.float64,     # init_T\n        types.float64,     # init_dif\n        types.float64,     # alpha_volume\n        types.float64,     # alpha_imbalance\n        types.Tuple((\n            types.float64,  # bar_open\n            types.float64,  # bar_high\n            types.float64,  # bar_low\n            types.float64,  # bar_close\n            types.float64,  # bar_start_time\n            types.float64,  # bar_end_time\n            types.float64,  # current_imbalance\n            types.float64,  # buy_volume_usd\n            types.float64,  # total_volume_usd\n            types.float64   # total_volume\n        )),\n        types.int64        # warm_up_count\n    )\n)\ndef process_partition_imbalance_numba(\n    prices, times, imbalances, sides, qtys,\n    init_T, init_dif, alpha_volume, alpha_imbalance, res_init, warm_up_count\n):\n    \"\"\"Processa uma partição usando numba para aceleração com período de warm-up.\"\"\"\n    exp_T = init_T\n    exp_dif = init_dif\n    threshold = exp_T * abs(exp_dif)\n    \n    # Contador de barras formadas (incluindo warm-up)\n    bars_formed = warm_up_count\n\n    bars = List()  # Lista tipada para armazenar as barras formadas\n    params = List()\n\n    # Desempacota res_init\n    bar_open, bar_high, bar_low, bar_close, bar_start_time, bar_end_time, \\\n    current_imbalance, buy_volume_usd, total_volume_usd, total_volume = res_init\n\n    # Verifica se res_init está inicializado (usando -1.0 como sentinela para não inicializado)\n    if bar_open == -1.0:\n        # Reseta as variáveis de agregação\n        bar_open = np.nan\n        bar_high = -np.inf\n        bar_low = np.inf\n        bar_close = np.nan\n        bar_start_time = np.nan\n        bar_end_time = np.nan\n        current_imbalance = 0.0\n        buy_volume_usd = 0.0\n        total_volume_usd = 0.0\n        total_volume = 0.0\n\n    for i in range(len(prices)):\n        if np.isnan(bar_open):\n            bar_open = prices[i]\n            bar_start_time = times[i]\n\n        trade_price = prices[i]\n        bar_high = max(bar_high, trade_price)\n        bar_low = min(bar_low, trade_price)\n        bar_close = trade_price\n\n        trade_imbalance = imbalances[i]\n\n        if sides[i] > 0:\n            buy_volume_usd += trade_imbalance\n\n        total_volume += qtys[i]\n        total_volume_usd += abs(trade_imbalance)\n        current_imbalance += trade_imbalance\n        imbalance = abs(current_imbalance)\n\n        if imbalance >= threshold:\n            bar_end_time = times[i]\n            bars_formed += 1\n\n            # Só salva a barra se já passou o período de warm-up (5 barras)\n            if bars_formed > 5:\n                bars.append((\n                    bar_start_time, bar_end_time, bar_open, bar_high, bar_low, bar_close,\n                    current_imbalance, buy_volume_usd, total_volume_usd, total_volume\n                ))\n\n            # Atualiza os valores exponenciais (sempre, mesmo durante warm-up)\n            if exp_dif == 1.0:\n                exp_T = total_volume_usd\n                exp_dif = abs(2 * buy_volume_usd / total_volume_usd - 1)\n            else:\n                exp_T += alpha_volume * (total_volume_usd - exp_T)\n                exp_dif += alpha_imbalance * (abs(2 * buy_volume_usd / total_volume_usd - 1) - exp_dif)\n\n            threshold = exp_T * abs(exp_dif)\n\n            # Só salva os parâmetros se já passou o período de warm-up\n            if bars_formed > 5:\n                params.append((\n                    exp_T, exp_dif, threshold\n                ))\n\n            # Reseta as variáveis de agregação\n            bar_open = np.nan\n            bar_high = -np.inf\n            bar_low = np.inf\n            bar_close = np.nan\n            bar_start_time = np.nan\n            bar_end_time = np.nan\n            current_imbalance = 0.0\n            buy_volume_usd = 0.0\n            total_volume_usd = 0.0\n            total_volume = 0.0\n\n    # Prepara o estado final para a próxima partição\n    final_state = (\n        bar_open, bar_high, bar_low, bar_close,\n        bar_start_time, bar_end_time, current_imbalance,\n        buy_volume_usd, total_volume_usd, total_volume\n    )\n\n    return bars, exp_T, exp_dif, final_state, params, bars_formed\n\ndef create_imbalance_dollar_bars_numba(partition, init_T, init_dif, res_init, alpha_volume, alpha_imbalance, warm_up_count):\n    \"\"\"Função wrapper para processar uma partição com numba.\"\"\"\n    # Converte a partição para arrays numpy\n    prices = partition['price'].values.astype(np.float64)\n    times = partition['time'].values.astype(np.float64)\n    imbalances = partition['dollar_imbalance'].values.astype(np.float64)\n    sides = partition['side'].values.astype(np.int8)\n    qtys = partition['qty'].values.astype(np.float64)\n\n    # Inicializa res_init se vazio ou inválido\n    if res_init is None or len(res_init) != 10:\n        res_init = (-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0)\n\n    # Processa a partição usando a função compilada com numba\n    bars, exp_T, exp_dif, res_init, params, warm_up_count = process_partition_imbalance_numba(\n        prices, times, imbalances, sides, qtys,\n        init_T, init_dif, alpha_volume, alpha_imbalance, res_init, warm_up_count\n    )\n\n    # Converte as barras para um DataFrame\n    if len(bars) > 0:\n        bars_df = pd.DataFrame(bars, columns=[\n            'start_time', 'end_time', 'open', 'high', 'low', 'close',\n            'imbalance_col', 'total_volume_buy_usd', 'total_volume_usd', 'total_volume'\n        ])\n        params_df = pd.DataFrame(params, columns=['ewma_volume', 'ewma_dif', 'thres'])\n    else:\n        # Retorna um DataFrame vazio com as colunas apropriadas\n        bars_df = pd.DataFrame(columns=[\n            'start_time', 'end_time', 'open', 'high', 'low', 'close',\n            'imbalance_col', 'total_volume_buy_usd', 'total_volume_usd', 'total_volume'\n        ])\n        params_df = pd.DataFrame(columns=['ewma_volume', 'ewma_dif', 'thres'])\n\n    return bars_df, exp_T, exp_dif, res_init, params_df, warm_up_count\n\ndef batch_create_imbalance_dollar_bars_optimized(df_dask, init_T, init_dif, res_init, alpha_volume, alpha_imbalance, warm_up_count=0):\n    \"\"\"Processa partições em lote para criar barras de desequilíbrio em dólares.\"\"\"\n    results = []\n    params_save = []\n    for partition in range(df_dask.npartitions):\n        logging.info(f'Processando partição {partition+1} de {df_dask.npartitions}')\n        part = df_dask.get_partition(partition).compute()\n\n        bars, init_T, init_dif, res_init, params, warm_up_count = create_imbalance_dollar_bars_numba(\n            part, init_T, init_dif, res_init, alpha_volume, alpha_imbalance, warm_up_count\n        )\n        results.append(bars)\n        params_save.append(params)\n    # Filtra DataFrames vazios\n    results = [df for df in results if not df.empty]\n    params_save = [df for df in params_save if not df.empty]\n    if results:\n        results_df = pd.concat(results, ignore_index=True)\n        params_df = pd.concat(params_save, ignore_index=True)\n    else:\n        # Retorna um DataFrame vazio com as colunas apropriadas se não houver resultados\n        results_df = pd.DataFrame(columns=[\n            'start_time', 'end_time', 'open', 'high', 'low', 'close',\n            'imbalance_col', 'total_volume_buy_usd', 'total_volume_usd', 'total_volume'\n        ])\n        params_df = pd.DataFrame(columns=['ewma_volume', 'ewma_dif', 'thres'])\n    return results_df, init_T, init_dif, res_init, params_df, warm_up_count"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Meta DataFrame para map_partitions\nmeta = pd.DataFrame({\n    'price': pd.Series(dtype='float32'),\n    'qty': pd.Series(dtype='float32'),\n    'quoteQty': pd.Series(dtype='float32'),\n    'time': pd.Series(dtype='float64'),  # Alterado para float64 para compatibilidade com Numba\n    'side': pd.Series(dtype='int8')\n})"
  },
  {
   "cell_type": "code",
   "source": "# Verificar se o diretório existe e listar arquivos disponíveis\nif os.path.exists(raw_dataset_path):\n    files = [f for f in os.listdir(raw_dataset_path) if f.endswith('.parquet')]\n    file_count = len(files)\n    print(f\"Encontrados {file_count} arquivos Parquet no diretório:\")\n    for i, f in enumerate(files[:5]):  # Mostra apenas os primeiros 5\n        print(f\"  {i+1}. {f}\")\n    if file_count > 5:\n        print(f\"  ... e mais {file_count - 5} arquivos\")\nelse:\n    print(f\"ERRO: O diretório {raw_dataset_path} não existe!\")\n    print(\"Verifique se você executou as etapas anteriores do pipeline:\")\n    print(\"  1. Download dos dados\")\n    print(\"  2. Conversão para Parquet\")\n    print(\"  3. Otimização dos arquivos Parquet\")\n    files = []\n    file_count = 0",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Apenas cria initial_state se houver arquivos\nif file_count > 0:\n    initial_state = [[init_T0, alpha_volume/100, alpha_imbalance/100, number]\n                      for init_T0 in range(1_000_000, 1_000_000_000, 200_000_000)\n                      for alpha_volume in range(10, 100, 25)\n                      for alpha_imbalance in range(10, 100, 25)\n                      for number in range(1, file_count)]\n\n    initial_state = initial_state[:file_count-1]\n    print(f\"Criados {len(initial_state)} estados iniciais para processamento\")\n    initial_state[:5]  # Mostra apenas os primeiros 5\nelse:\n    initial_state = []\n    print(\"Nenhum arquivo encontrado para processar!\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "processing_times = {}\n\n# Verifica se há estados para processar\nif not initial_state:\n    print(\"Nenhum estado inicial para processar. Verifique se há arquivos no diretório de dados.\")\nelse:\n    for init_T0, alpha_volume, alpha_imbalance, number in initial_state:\n        if number == 1:\n            start_time = time.time()\n            output_file = f'imbalance_dolar_{init_T0}-{alpha_volume}-{alpha_imbalance}'\n            results = pd.DataFrame()\n            params = pd.DataFrame()\n            init_dif = 1.0\n            res_init = (-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0)  # Estado inicial\n            init_T = init_T0\n            warm_up_count = 0  # Inicializa o contador de warm-up\n            logging.info(f\"params_{output_file}\")\n\n        number = str(number).zfill(3)\n        file = f'BTCUSDT-Trades-Optimized-{number}.parquet'\n\n        logging.info(f\"Dask n{number} de {file_count-1}\")\n\n        if not os.path.exists(os.path.join(raw_dataset_path, file)):\n            logging.warning(f\"Arquivo {file} não encontrado. Pulando para o próximo.\")\n            continue\n\n        df_dask = read_parquet_files_optimized(raw_dataset_path, file)\n\n        df_dask = apply_operations_optimized(df_dask, meta)\n\n        bars, init_T, init_dif, res_init, params_df, warm_up_count = batch_create_imbalance_dollar_bars_optimized(\n            df_dask, init_T, init_dif, res_init, alpha_volume, alpha_imbalance, warm_up_count\n        )\n        results = pd.concat([results, bars], ignore_index=True)\n        params = pd.concat([params, params_df], ignore_index=True)\n\n        if number == str(file_count - 1):\n            bar_open, bar_high, bar_low, bar_close, bar_start_time, bar_end_time, \\\n            current_imbalance, buy_volume_usd, total_volume_usd, total_volume = res_init\n\n            bar_end_time = df_dask['time'].tail().iloc[-1]\n\n            lastbar = [[bar_start_time, bar_end_time, bar_open, bar_high, bar_low, bar_close,\n                            current_imbalance, buy_volume_usd, total_volume_usd, total_volume]]\n\n            lastbar = pd.DataFrame(lastbar, columns=['start_time', 'end_time', 'open', 'high', 'low', 'close', 'imbalance_col', 'total_volume_buy_usd', 'total_volume_usd', 'total_volume'])\n\n            results = pd.concat([results, lastbar], ignore_index=True)\n\n            results_ = results.copy()\n\n            # results_['start_time'] = pd.to_datetime(results_['start_time'], unit='ns')  # Ajuste para 's' se 'time' era em segundos\n            # results_['end_time'] = pd.to_datetime(results_['end_time'], unit='ns')\n\n            results_['start_time'] = pd.to_datetime(results_['start_time'])  # Ajuste para 's' se 'time' era em segundos\n            results_['end_time'] = pd.to_datetime(results_['end_time'])\n            results_.drop(columns=['start_time'], inplace=True)\n\n            results_['params'] = output_file\n\n            results_['time_trial'] = timestamp\n            \n            # Salva o arquivo Excel no diretório de output\n            output_excel_path = os.path.join(output_base_path, f'{output_file}.xlsx')\n            results_.to_excel(output_excel_path, index=False)\n            logging.info(f\"Arquivo salvo em: {output_excel_path}\")\n            logging.info(f\"Total de barras formadas (incluindo warm-up): {warm_up_count}\")\n            logging.info(f\"Total de barras salvas (excluindo warm-up): {len(results_)}\")\n\n            # df_eq = pd.DataFrame([[init_T, init_dif, output_file]], columns=['v-ewma', 'imbalance-ewma', 'params'])\n\n            # # host = \"superset-postgresql.default.svc.cluster.local\"  # Nome do serviço no Kubernetes÷\n            # host = \"localhost\"  # Agora o PostgreSQL está acessível via localhost\n            # port = 5432  # Porta do PostgreSQL\n            # dbname = \"superset\"  # Nome do banco de dados\n            # user = \"superset\"  # Usuário do banco de dados\n            # password = \"superset\"  # Senha do banco de dados\n\n\n            # # Conectar ao banco de dados PostgreSQL usando SQLAlchemy\n            # connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{dbname}\"\n\n            # with create_engine(connection_string).connect() as connection:\n            #     # Enviar o DataFrame para o PostgreSQL\n            #     results_.to_sql('imbalance-bars-start', connection, if_exists='append', index=False)\n\n            #     print(\"Dados enviados para o banco de dados com sucesso!\")\n            #     # Finaliza a medição do tempo\n\n            end_time = time.time()\n            elapsed_time = end_time - start_time\n            elapsed_time_minutes = elapsed_time / 60  # Converte para minutos\n            # Armazena o tempo de processamento\n            processing_times[file] = elapsed_time_minutes\n            logging.info(f\"Tempo de processamento para {file}: {elapsed_time_minutes:.2f} minutos\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "processing_times = {}\n\nfor init_T0, alpha_volume, alpha_imbalance, number in initial_state:\n    if number == 1:\n        start_time = time.time()\n        output_file = f'imbalance_dolar_{init_T0}-{alpha_volume}-{alpha_imbalance}'\n        results = pd.DataFrame()\n        params = pd.DataFrame()\n        init_dif = 1.0\n        res_init = (-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 0.0, 0.0, 0.0, 0.0)  # Estado inicial\n        init_T = init_T0\n        warm_up_count = 0  # Inicializa o contador de warm-up\n        logging.info(f\"params_{output_file}\")\n\n    number = str(number).zfill(3)\n    file = f'BTCUSDT-Trades-Optimized-{number}.parquet'\n\n    logging.info(f\"Dask n{number} de {file_count-1}\")\n\n    if not os.path.exists(os.path.join(raw_dataset_path, file)):\n        logging.warning(f\"Arquivo {file} não encontrado. Pulando para o próximo.\")\n        continue\n\n    df_dask = read_parquet_files_optimized(raw_dataset_path, file)\n\n    df_dask = apply_operations_optimized(df_dask, meta)\n\n    bars, init_T, init_dif, res_init, params_df, warm_up_count = batch_create_imbalance_dollar_bars_optimized(\n        df_dask, init_T, init_dif, res_init, alpha_volume, alpha_imbalance, warm_up_count\n    )\n    results = pd.concat([results, bars], ignore_index=True)\n    params = pd.concat([params, params_df], ignore_index=True)\n\n    if number == str(file_count - 1):\n        bar_open, bar_high, bar_low, bar_close, bar_start_time, bar_end_time, \\\n        current_imbalance, buy_volume_usd, total_volume_usd, total_volume = res_init\n\n        bar_end_time = df_dask['time'].tail().iloc[-1]\n\n        lastbar = [[bar_start_time, bar_end_time, bar_open, bar_high, bar_low, bar_close,\n                        current_imbalance, buy_volume_usd, total_volume_usd, total_volume]]\n\n        lastbar = pd.DataFrame(lastbar, columns=['start_time', 'end_time', 'open', 'high', 'low', 'close', 'imbalance_col', 'total_volume_buy_usd', 'total_volume_usd', 'total_volume'])\n\n        results = pd.concat([results, lastbar], ignore_index=True)\n\n        results_ = results.copy()\n\n        # results_['start_time'] = pd.to_datetime(results_['start_time'], unit='ns')  # Ajuste para 's' se 'time' era em segundos\n        # results_['end_time'] = pd.to_datetime(results_['end_time'], unit='ns')\n\n        results_['start_time'] = pd.to_datetime(results_['start_time'])  # Ajuste para 's' se 'time' era em segundos\n        results_['end_time'] = pd.to_datetime(results_['end_time'])\n        results_.drop(columns=['start_time'], inplace=True)\n\n        results_['params'] = output_file\n\n        results_['time_trial'] = timestamp\n        results_.to_excel(f'../output/{output_file}.xlsx', index=False)\n        logging.info(f\"Arquivo salvo em: ../output/{output_file}.xlsx\")\n        logging.info(f\"Total de barras formadas (incluindo warm-up): {warm_up_count}\")\n        logging.info(f\"Total de barras salvas (excluindo warm-up): {len(results_)}\")\n\n        # df_eq = pd.DataFrame([[init_T, init_dif, output_file]], columns=['v-ewma', 'imbalance-ewma', 'params'])\n\n        # # host = \"superset-postgresql.default.svc.cluster.local\"  # Nome do serviço no Kubernetes÷\n        # host = \"localhost\"  # Agora o PostgreSQL está acessível via localhost\n        # port = 5432  # Porta do PostgreSQL\n        # dbname = \"superset\"  # Nome do banco de dados\n        # user = \"superset\"  # Usuário do banco de dados\n        # password = \"superset\"  # Senha do banco de dados\n\n\n        # # Conectar ao banco de dados PostgreSQL usando SQLAlchemy\n        # connection_string = f\"postgresql://{user}:{password}@{host}:{port}/{dbname}\"\n\n        # with create_engine(connection_string).connect() as connection:\n        #     # Enviar o DataFrame para o PostgreSQL\n        #     results_.to_sql('imbalance-bars-start', connection, if_exists='append', index=False)\n\n        #     print(\"Dados enviados para o banco de dados com sucesso!\")\n        #     # Finaliza a medição do tempo\n\n        end_time = time.time()\n        elapsed_time = end_time - start_time\n        elapsed_time_minutes = elapsed_time / 60  # Converte para minutos\n        # Armazena o tempo de processamento\n        processing_times[file] = elapsed_time_minutes\n        logging.info(f\"Tempo de processamento para {file}: {elapsed_time_minutes:.2f} minutos\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}