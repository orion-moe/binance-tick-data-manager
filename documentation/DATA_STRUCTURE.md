# Data Directory Structure

This directory contains all trading data organized by ticker and market type.

## Organization

Each ticker gets its own directory following this pattern:
```
{symbol}-{market_type}/
```

Examples:
- `btcusdt-spot/` - BTCUSDT spot market
- `btcusdt-futures-um/` - BTCUSDT USD-M futures
- `ethusdt-spot/` - ETHUSDT spot market

## Subdirectory Structure

Inside each ticker directory:

```
btcusdt-spot/
â”œâ”€â”€ raw-zip-daily/              # Downloaded ZIP/CSV files
â”œâ”€â”€ raw-parquet-daily/          # Processed Parquet files (1:1 from ZIPs, snappy compressed)
â”œâ”€â”€ raw-parquet-merged-daily/         # Large merged Parquet files (~10GB each, snappy compressed)
â”œâ”€â”€ logs/                       # Download and processing logs
â”œâ”€â”€ download_progress_daily.json    # Progress tracking for downloads
â””â”€â”€ failed_downloads.txt        # List of failed download attempts
```

## Directory Names Explained

### Raw Data (Input)
- **`raw-zip-daily/`** - Original ZIP files downloaded from Binance

### Processed Data (Intermediate)
- **`raw-parquet-daily/`** - ZIP files converted to Parquet format (one file per day, snappy compressed)

### Optimized Data (Output)
- **`raw-parquet-merged-daily/`** - Parquet files merged and optimized into ~10GB files
  - Combines multiple daily files
  - Uses Snappy compression (fast read/write)
  - Optimized row group size for ML workloads

## Parquet Compression

All Parquet files use **Snappy compression**:
- âœ… **Fast**: Best balance between compression ratio and speed
- âœ… **Reliable**: Industry standard, widely supported
- âœ… **Consistent**: Same compression used throughout the pipeline

**Once set, compression is automatically maintained** - you don't need to specify it again!

### Why Snappy?

| Compression | Speed | Ratio | Use Case |
|-------------|-------|-------|----------|
| **Snappy** | âš¡âš¡âš¡ Fast | ~2-3x | **ML/Analytics** â† We use this |
| GZIP | ğŸŒ Slow | ~5-10x | Long-term storage |
| LZ4 | âš¡âš¡âš¡ Fastest | ~1.5-2x | Real-time streaming |
| Zstd | âš¡âš¡ Medium | ~3-7x | General purpose |

**Snappy is optimal for:**
- Fast data loading during training
- Quick exploratory analysis
- High-throughput ML pipelines

## Git Protection

All files in this directory are automatically ignored by git via `.gitignore`:
- âœ… No raw data will ever be committed
- âœ… No progress files will be committed
- âœ… Only the directory structure is tracked

## Usage

When you run:
```bash
python main.py download --symbol BTCUSDT --type spot --granularity daily
```

Data will be stored in:
```
data/btcusdt-spot/raw-zip-daily/       # Downloaded ZIP files
data/btcusdt-spot/raw-parquet-daily/   # Processed parquet files (snappy)
data/btcusdt-spot/logs/                # Download logs
```

## Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Download                                          â”‚
â”‚    raw-zip-daily/ (100-200MB per file)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Convert to Parquet (snappy compression)           â”‚
â”‚    raw-parquet-daily/ (50-100MB per file)            â”‚
â”‚    â€¢ 1 parquet per day                               â”‚
â”‚    â€¢ Snappy compression applied automatically        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Merge & Optimize (snappy compression maintained)  â”‚
â”‚    raw-parquet-merged-daily/ (~10GB per file)              â”‚
â”‚    â€¢ Combines multiple files                         â”‚
â”‚    â€¢ Same snappy compression                         â”‚
â”‚    â€¢ Optimized for ML workloads                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Benefits of This Structure

1. **Clear Naming**: Easy to understand what each folder contains
   - `raw-zip-daily/` = original downloads
   - `raw-parquet-daily/` = converted with snappy compression
   - `raw-parquet-merged-daily/` = final merged files (snappy)

2. **Isolated**: Each ticker's data is completely separated

3. **Scalable**: Easy to add new tickers without affecting existing ones

4. **Safe**: Git will never track your data files

5. **Optimized**: Snappy compression throughout for best ML performance

6. **Automatic**: Compression is set once and maintained automatically

## Example: BTCUSDT Spot Data

After downloading one year of daily data:

```
data/btcusdt-spot/
â”œâ”€â”€ raw-zip-daily/
â”‚   â”œâ”€â”€ BTCUSDT-trades-2024-01-01.zip (200MB)
â”‚   â”œâ”€â”€ BTCUSDT-trades-2024-01-02.zip (200MB)
â”‚   â””â”€â”€ ... (365 files)
â”‚
â”œâ”€â”€ raw-parquet-daily/
â”‚   â”œâ”€â”€ BTCUSDT-Trades-2024-01-01.parquet (65MB, snappy)
â”‚   â”œâ”€â”€ BTCUSDT-Trades-2024-01-02.parquet (65MB, snappy)
â”‚   â””â”€â”€ ... (365 files)
â”‚
â””â”€â”€ raw-parquet-merged-daily/
    â”œâ”€â”€ BTCUSDT-Trades-Optimized-001.parquet (10GB, snappy)
    â”œâ”€â”€ BTCUSDT-Trades-Optimized-002.parquet (10GB, snappy)
    â””â”€â”€ BTCUSDT-Trades-Optimized-003.parquet (4GB, snappy)
```

**Result**: 365 days compressed into 3 optimized files ready for ML!
