# CompressÃ£o Parquet - Guia Completo

## âœ… RESPOSTA RÃPIDA

**SIM, a compressÃ£o Ã© definida uma vez e reutilizada automaticamente em TODOS os arquivos Parquet!**

Uma vez que vocÃª escolhe **Snappy** (que jÃ¡ estÃ¡ configurado), todos os Parquet files usarÃ£o Snappy automaticamente:
- âœ… `raw-parquet-daily/` â†’ Snappy
- âœ… `raw-parquet-monthly/` â†’ Snappy
- âœ… `raw-parquet-merged/` â†’ Snappy

**VocÃª NÃƒO precisa especificar a compressÃ£o novamente!**

---

## ğŸ”§ CompressÃ£o Configurada: Snappy

### Onde estÃ¡ configurado?

**Arquivo**: `src/data_pipeline/downloaders/binance_downloader.py`

```python
# Linha 619 - CriaÃ§Ã£o de arquivos Parquet individuais
writer = pq.ParquetWriter(output_path, table.schema, compression='snappy')

# Linha 1573 - Merge de arquivos Parquet
writer = pq.ParquetWriter(last_optimized, schema, compression='snappy')

# Linha 1581 - CriaÃ§Ã£o de novos arquivos merged
writer = pq.ParquetWriter(current_output, schema, compression='snappy')
```

**Resultado**: Todos os Parquet files usam Snappy!

---

## ğŸ“Š Como Funciona a CompressÃ£o Parquet

### CompressÃ£o Ã© uma Propriedade do Arquivo

Quando vocÃª cria um arquivo Parquet:

```python
writer = pq.ParquetWriter(
    'arquivo.parquet',
    schema,
    compression='snappy'  # â† Definido aqui, uma Ãºnica vez
)
```

A compressÃ£o fica **embutida no arquivo**. Qualquer ferramenta que ler o arquivo:
- âœ… Detecta automaticamente que Ã© Snappy
- âœ… Descomprime automaticamente ao ler
- âœ… **NÃƒO precisa** especificar compressÃ£o ao ler

### Exemplo PrÃ¡tico

```python
# ESCREVER (vocÃª especifica)
import pyarrow.parquet as pq
table = pa.Table.from_pandas(df)
pq.write_table(table, 'dados.parquet', compression='snappy')

# LER (automÃ¡tico, nÃ£o precisa especificar!)
df = pd.read_parquet('dados.parquet')  # Descomprime sozinho!
```

---

## ğŸ¯ Por Que Usamos Snappy?

### ComparaÃ§Ã£o de CompressÃµes

| CompressÃ£o | Velocidade Leitura | Velocidade Escrita | Taxa CompressÃ£o | Tamanho Final |
|------------|-------------------|-------------------|----------------|---------------|
| **Snappy** | âš¡âš¡âš¡ 500 MB/s | âš¡âš¡âš¡ 250 MB/s | ~3x | 100 MB â†’ 33 MB |
| GZIP | ğŸŒ 100 MB/s | ğŸŒ 20 MB/s | ~10x | 100 MB â†’ 10 MB |
| LZ4 | âš¡âš¡âš¡âš¡ 700 MB/s | âš¡âš¡âš¡ 300 MB/s | ~2x | 100 MB â†’ 50 MB |
| Zstd | âš¡âš¡ 300 MB/s | âš¡âš¡ 100 MB/s | ~7x | 100 MB â†’ 14 MB |
| None | âš¡âš¡âš¡âš¡âš¡ 1000 MB/s | âš¡âš¡âš¡âš¡âš¡ 900 MB/s | 1x | 100 MB â†’ 100 MB |

### Por Que Snappy Ã© Ideal para ML?

**Caso de Uso: Treinar modelo com 100GB de dados**

| CompressÃ£o | Tempo Leitura | Tamanho em Disco | Velocidade Treinamento |
|------------|--------------|------------------|----------------------|
| None | 2 min | 100 GB | âš¡ Muito RÃ¡pido |
| **Snappy** | **3 min** | **33 GB** | **âš¡ RÃ¡pido** â† **MELHOR** |
| GZIP | 15 min | 10 GB | ğŸŒ Lento |
| Zstd | 6 min | 14 GB | âš¡ MÃ©dio |

**Snappy vence porque:**
1. âœ… Economiza 67% de espaÃ§o em disco
2. âœ… Adiciona apenas 50% no tempo de leitura
3. âœ… NÃ£o sacrifica velocidade de treinamento
4. âœ… Ã‰ o padrÃ£o da indÃºstria para ML

---

## ğŸ”„ Fluxo de CompressÃ£o no Pipeline

### Etapa 1: Download (ZIP da Binance)

```
Binance Server
     â†“
raw-zip-daily/BTCUSDT-trades-2024-01-15.zip (200 MB)
```

**CompressÃ£o**: ZIP (compressÃ£o da Binance)

---

### Etapa 2: ConversÃ£o para Parquet (Snappy aplicado)

```python
# binance_downloader.py:619
writer = pq.ParquetWriter(output_path, table.schema, compression='snappy')
```

```
raw-zip-daily/BTCUSDT-trades-2024-01-15.zip (200 MB)
     â†“ [Extrai CSV]
     â†“ [Converte para Parquet + Snappy]
raw-parquet-daily/BTCUSDT-Trades-2024-01-15.parquet (65 MB)
```

**CompressÃ£o**: Snappy (definida aqui)

---

### Etapa 3: Merge (Snappy mantido)

```python
# binance_downloader.py:1581
writer = pq.ParquetWriter(current_output, schema, compression='snappy')
```

```
raw-parquet-daily/
â”œâ”€â”€ BTCUSDT-Trades-2024-01-01.parquet (65 MB, snappy)
â”œâ”€â”€ BTCUSDT-Trades-2024-01-02.parquet (65 MB, snappy)
â”œâ”€â”€ ...
â””â”€â”€ BTCUSDT-Trades-2024-12-31.parquet (65 MB, snappy)
     â†“ [Merge com Snappy]
raw-parquet-merged/BTCUSDT-Trades-Optimized-001.parquet (10 GB, snappy)
```

**CompressÃ£o**: Snappy (mesma)

---

## âœ… Garantias

### 1. ConsistÃªncia Total

âœ… **TODOS** os arquivos Parquet usam Snappy
âœ… NÃ£o hÃ¡ mistura de compressÃµes
âœ… Pipeline totalmente consistente

### 2. AutomÃ¡tico

âœ… Definido uma vez no cÃ³digo
âœ… Reutilizado automaticamente
âœ… VocÃª **NÃƒO** precisa pensar nisso!

### 3. Compatibilidade

âœ… Qualquer ferramenta Parquet lÃª corretamente:
- âœ… Pandas: `pd.read_parquet()`
- âœ… Dask: `dd.read_parquet()`
- âœ… PyArrow: `pq.read_table()`
- âœ… Spark: `spark.read.parquet()`
- âœ… DuckDB: `SELECT * FROM parquet_scan()`

Todas detectam Snappy automaticamente!

---

## ğŸ§ª Como Verificar a CompressÃ£o

### Verificar arquivo Parquet:

```python
import pyarrow.parquet as pq

# Ler metadados
pf = pq.ParquetFile('data/btcusdt-spot/raw-parquet-daily/BTCUSDT-Trades-2024-01-15.parquet')

# Ver compressÃ£o
for i in range(pf.metadata.num_row_groups):
    rg = pf.metadata.row_group(i)
    for j in range(rg.num_columns):
        col = rg.column(j)
        print(f"Column {col.path_in_schema}: {col.compression}")
```

**Output esperado**:
```
Column trade_id: SNAPPY
Column price: SNAPPY
Column qty: SNAPPY
...
```

### Via Terminal:

```bash
# Instalar parquet-tools
pip install parquet-tools

# Ver metadados
parquet-tools meta data/btcusdt-spot/raw-parquet-daily/BTCUSDT-Trades-2024-01-15.parquet

# Output mostrarÃ¡: compression: SNAPPY
```

---

## ğŸ”§ Se Precisar Mudar a CompressÃ£o

### CenÃ¡rio: VocÃª quer usar GZIP em vez de Snappy

**Arquivo**: `binance_downloader.py`

```python
# Substituir em 3 lugares:

# Linha 619
writer = pq.ParquetWriter(output_path, table.schema, compression='gzip')

# Linha 1573
writer = pq.ParquetWriter(last_optimized, schema, compression='gzip')

# Linha 1581
writer = pq.ParquetWriter(current_output, schema, compression='gzip')
```

**Mas NÃƒO recomendamos!** Snappy Ã© ideal para ML.

---

## ğŸ“ Resumo

### Perguntas e Respostas

**Q: A compressÃ£o Ã© reutilizada automaticamente?**
A: âœ… SIM! Uma vez definida (`compression='snappy'`), todos os arquivos usam Snappy.

**Q: Preciso especificar ao ler?**
A: âŒ NÃƒO! Pandas/PyArrow detectam automaticamente.

**Q: Posso misturar compressÃµes?**
A: âš ï¸ Tecnicamente sim, mas **nÃ£o faÃ§a isso!** Mantenha consistÃªncia.

**Q: Snappy Ã© a melhor escolha?**
A: âœ… SIM para ML! Melhor balanÃ§o velocidade/tamanho.

**Q: Os arquivos jÃ¡ existentes usam Snappy?**
A: âœ… SIM! Veja cÃ³digo nas linhas 619, 1573, 1581.

---

## ğŸ¯ ConclusÃ£o

### ConfiguraÃ§Ã£o Atual (Ã“tima para ML):

```
Pipeline Completo:
â”œâ”€â”€ raw-zip-daily/        (ZIP da Binance)
â”œâ”€â”€ raw-parquet-daily/    (Parquet + Snappy) âœ…
â””â”€â”€ raw-parquet-merged/   (Parquet + Snappy) âœ…
```

**Resultado**:
- âœ… 67% de economia de espaÃ§o vs nÃ£o comprimido
- âœ… 3x menor que CSV
- âœ… RÃ¡pido para treinar modelos
- âœ… Totalmente automÃ¡tico
- âœ… Consistente em todo o pipeline

**VocÃª nÃ£o precisa se preocupar com compressÃ£o - jÃ¡ estÃ¡ otimizado!** ğŸš€
