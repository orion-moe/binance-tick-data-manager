{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:<Client: 'tcp://127.0.0.1:53865' processes=10 threads=10, memory=59.60 GiB>\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-1.parquet, Tamanho: 3.01 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-2.parquet, Tamanho: 3.08 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-3.parquet, Tamanho: 3.03 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-4.parquet, Tamanho: 2.90 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-5.parquet, Tamanho: 2.78 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-6.parquet, Tamanho: 2.97 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-7.parquet, Tamanho: 2.95 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-8.parquet, Tamanho: 2.91 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-9.parquet, Tamanho: 2.91 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-10.parquet, Tamanho: 2.65 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-11.parquet, Tamanho: 2.56 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-12.parquet, Tamanho: 2.63 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-13.parquet, Tamanho: 2.52 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-14.parquet, Tamanho: 2.54 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-15.parquet, Tamanho: 2.51 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-16.parquet, Tamanho: 2.44 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-17.parquet, Tamanho: 2.46 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-18.parquet, Tamanho: 2.55 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-19.parquet, Tamanho: 2.47 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-20.parquet, Tamanho: 2.37 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-21.parquet, Tamanho: 2.43 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-22.parquet, Tamanho: 2.34 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-23.parquet, Tamanho: 2.34 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-24.parquet, Tamanho: 2.35 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-25.parquet, Tamanho: 2.32 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-26.parquet, Tamanho: 2.30 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-27.parquet, Tamanho: 2.14 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-28.parquet, Tamanho: 2.25 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-29.parquet, Tamanho: 2.36 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-30.parquet, Tamanho: 2.37 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-31.parquet, Tamanho: 2.27 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-32.parquet, Tamanho: 2.21 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-33.parquet, Tamanho: 2.15 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-34.parquet, Tamanho: 2.16 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-35.parquet, Tamanho: 2.06 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-36.parquet, Tamanho: 1.90 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-37.parquet, Tamanho: 1.25 GB\n",
      "INFO:root:Arquivo: BTCUSDT-Dataset-part-38.parquet, Tamanho: 6.54 GB\n",
      "INFO:root:Tamanho total dos arquivos: 97.95 GB\n",
      "INFO:root:Novo arquivo 1: ['BTCUSDT-Dataset-part-1.parquet', 'BTCUSDT-Dataset-part-2.parquet', 'BTCUSDT-Dataset-part-3.parquet'], Tamanho: 9.11 GB\n",
      "INFO:root:Novo arquivo 2: ['BTCUSDT-Dataset-part-4.parquet', 'BTCUSDT-Dataset-part-5.parquet', 'BTCUSDT-Dataset-part-6.parquet'], Tamanho: 8.66 GB\n",
      "INFO:root:Novo arquivo 3: ['BTCUSDT-Dataset-part-7.parquet', 'BTCUSDT-Dataset-part-8.parquet', 'BTCUSDT-Dataset-part-9.parquet'], Tamanho: 8.76 GB\n",
      "INFO:root:Novo arquivo 4: ['BTCUSDT-Dataset-part-10.parquet', 'BTCUSDT-Dataset-part-11.parquet', 'BTCUSDT-Dataset-part-12.parquet'], Tamanho: 7.84 GB\n",
      "INFO:root:Novo arquivo 5: ['BTCUSDT-Dataset-part-13.parquet', 'BTCUSDT-Dataset-part-14.parquet', 'BTCUSDT-Dataset-part-15.parquet'], Tamanho: 7.56 GB\n",
      "INFO:root:Novo arquivo 6: ['BTCUSDT-Dataset-part-16.parquet', 'BTCUSDT-Dataset-part-17.parquet', 'BTCUSDT-Dataset-part-18.parquet', 'BTCUSDT-Dataset-part-19.parquet'], Tamanho: 9.92 GB\n",
      "INFO:root:Novo arquivo 7: ['BTCUSDT-Dataset-part-20.parquet', 'BTCUSDT-Dataset-part-21.parquet', 'BTCUSDT-Dataset-part-22.parquet', 'BTCUSDT-Dataset-part-23.parquet'], Tamanho: 9.48 GB\n",
      "INFO:root:Novo arquivo 8: ['BTCUSDT-Dataset-part-24.parquet', 'BTCUSDT-Dataset-part-25.parquet', 'BTCUSDT-Dataset-part-26.parquet', 'BTCUSDT-Dataset-part-27.parquet'], Tamanho: 9.11 GB\n",
      "INFO:root:Novo arquivo 9: ['BTCUSDT-Dataset-part-28.parquet', 'BTCUSDT-Dataset-part-29.parquet', 'BTCUSDT-Dataset-part-30.parquet', 'BTCUSDT-Dataset-part-31.parquet'], Tamanho: 9.25 GB\n",
      "INFO:root:Novo arquivo 10: ['BTCUSDT-Dataset-part-32.parquet', 'BTCUSDT-Dataset-part-33.parquet', 'BTCUSDT-Dataset-part-34.parquet', 'BTCUSDT-Dataset-part-35.parquet'], Tamanho: 8.58 GB\n",
      "INFO:root:Novo arquivo 11: ['BTCUSDT-Dataset-part-36.parquet', 'BTCUSDT-Dataset-part-37.parquet', 'BTCUSDT-Dataset-part-38.parquet'], Tamanho: 9.69 GB\n",
      "INFO:root:<Client: 'tcp://127.0.0.1:53865' processes=10 threads=10, memory=59.60 GiB>\n",
      "INFO:root:Partições lidas com sucesso.\n",
      "INFO:root:Arquivo combinado salvo: ./BTCUSDT-Trades-compressed/combined_file_1.parquet\n",
      "INFO:root:<Client: 'tcp://127.0.0.1:53865' processes=10 threads=10, memory=59.60 GiB>\n",
      "INFO:root:Partições lidas com sucesso.\n",
      "INFO:root:Arquivo combinado salvo: ./BTCUSDT-Trades-compressed/combined_file_2.parquet\n",
      "INFO:root:<Client: 'tcp://127.0.0.1:53865' processes=10 threads=10, memory=59.60 GiB>\n",
      "INFO:root:Partições lidas com sucesso.\n",
      "INFO:root:Arquivo combinado salvo: ./BTCUSDT-Trades-compressed/combined_file_3.parquet\n",
      "INFO:root:<Client: 'tcp://127.0.0.1:53865' processes=10 threads=10, memory=59.60 GiB>\n",
      "INFO:root:Partições lidas com sucesso.\n",
      "INFO:root:Arquivo combinado salvo: ./BTCUSDT-Trades-compressed/combined_file_4.parquet\n",
      "INFO:root:<Client: 'tcp://127.0.0.1:53865' processes=10 threads=10, memory=59.60 GiB>\n",
      "INFO:root:Partições lidas com sucesso.\n",
      "INFO:root:Arquivo combinado salvo: ./BTCUSDT-Trades-compressed/combined_file_5.parquet\n",
      "INFO:root:<Client: 'tcp://127.0.0.1:53865' processes=10 threads=10, memory=59.60 GiB>\n",
      "INFO:root:Partições lidas com sucesso.\n",
      "INFO:root:Arquivo combinado salvo: ./BTCUSDT-Trades-compressed/combined_file_6.parquet\n",
      "INFO:root:<Client: 'tcp://127.0.0.1:53865' processes=10 threads=10, memory=59.60 GiB>\n",
      "INFO:root:Partições lidas com sucesso.\n",
      "INFO:root:Arquivo combinado salvo: ./BTCUSDT-Trades-compressed/combined_file_7.parquet\n",
      "INFO:root:<Client: 'tcp://127.0.0.1:53865' processes=10 threads=10, memory=59.60 GiB>\n",
      "INFO:root:Partições lidas com sucesso.\n",
      "INFO:root:Arquivo combinado salvo: ./BTCUSDT-Trades-compressed/combined_file_8.parquet\n",
      "INFO:root:<Client: 'tcp://127.0.0.1:53865' processes=10 threads=10, memory=59.60 GiB>\n",
      "INFO:root:Partições lidas com sucesso.\n",
      "INFO:root:Arquivo combinado salvo: ./BTCUSDT-Trades-compressed/combined_file_9.parquet\n",
      "INFO:root:<Client: 'tcp://127.0.0.1:53865' processes=10 threads=10, memory=59.60 GiB>\n",
      "INFO:root:Partições lidas com sucesso.\n",
      "INFO:root:Arquivo combinado salvo: ./BTCUSDT-Trades-compressed/combined_file_10.parquet\n",
      "INFO:root:<Client: 'tcp://127.0.0.1:53865' processes=10 threads=10, memory=59.60 GiB>\n",
      "INFO:root:Partições lidas com sucesso.\n",
      "INFO:root:Arquivo combinado salvo: ./BTCUSDT-Trades-compressed/combined_file_11.parquet\n",
      "INFO:root:Client Dask fechado.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "import gc\n",
    "import dask.dataframe as dd\n",
    "import datetime\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask.distributed import Client, progress\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Configurar o logging para visualizar os logs de informação\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Configurar o cliente Dask\n",
    "client = Client(n_workers=10, threads_per_worker=1, memory_limit='6.4GB')\n",
    "logging.info(client)\n",
    "\n",
    "raw_dataset_path = './BTCUSDT-Trades/'\n",
    "output_base_path = './BTCUSDT-Trades-compressed/'\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "output_path = f'{output_base_path}'\n",
    "\n",
    "# Função para extrair o número do nome do arquivo\n",
    "def extract_number(file_name):\n",
    "    if file_name.startswith(\"BTCUSDT-Dataset-part-\") and file_name.endswith(\".parquet\"):\n",
    "        return int(file_name.split(\"-\")[-1].split(\".\")[0])  # Extrai o número\n",
    "    return -1  # Para arquivos que não seguem o padrão (como .DS_Store)\n",
    "\n",
    "# Função para converter tamanho em bytes para gigabytes (GB)\n",
    "def convert_to_gb(size_bytes):\n",
    "    return size_bytes / (1024 ** 3)  # 1 GB = 1024^3 bytes\n",
    "\n",
    "\n",
    "# Listar arquivos\n",
    "# files = [f for f in os.listdir(raw_dataset_path) if os.path.isfile(os.path.join(raw_dataset_path, f))]\n",
    "\n",
    "# Listar arquivos\n",
    "files = [f for f in os.listdir(raw_dataset_path) if os.path.isfile(os.path.join(raw_dataset_path, f))]\n",
    "file_count = len(files)\n",
    "\n",
    "\n",
    "files = [f for f in os.listdir(raw_dataset_path) if f != '.DS_Store']\n",
    "\n",
    "# Ordenar a lista\n",
    "sorted_files = sorted(files, key=extract_number)\n",
    "\n",
    "# Lista para armazenar o nome e o tamanho dos arquivos em GB\n",
    "file_sizes_gb = []\n",
    "\n",
    "# Ler o tamanho de cada arquivo em GB\n",
    "for file in sorted_files:\n",
    "    file_path = os.path.join(raw_dataset_path, file)\n",
    "    file_size_bytes = os.path.getsize(file_path)\n",
    "    file_size_gb = convert_to_gb(file_size_bytes)\n",
    "    file_sizes_gb.append((file, file_size_gb))\n",
    "    logging.info(f\"Arquivo: {file}, Tamanho: {file_size_gb:.2f} GB\")\n",
    "\n",
    "# Calcular o tamanho total dos arquivos em GB\n",
    "total_size_gb = sum(size for _, size in file_sizes_gb)\n",
    "logging.info(f\"Tamanho total dos arquivos: {total_size_gb:.2f} GB\")\n",
    "\n",
    "# Agrupar arquivos em novos arquivos de até 16 GB\n",
    "max_size_per_file_gb = 10  # Tamanho máximo por arquivo\n",
    "new_files = []  # Lista para armazenar os grupos de arquivos\n",
    "current_group = []  # Grupo atual de arquivos\n",
    "current_group_size = 0  # Tamanho do grupo atual em GB\n",
    "\n",
    "for file, size_gb in file_sizes_gb:\n",
    "    if current_group_size + size_gb <= max_size_per_file_gb:\n",
    "        current_group.append(file)\n",
    "        current_group_size += size_gb\n",
    "    else:\n",
    "        new_files.append((current_group, current_group_size))\n",
    "        current_group = [file]\n",
    "        current_group_size = size_gb\n",
    "\n",
    "# Adicionar o último grupo, se houver\n",
    "if current_group:\n",
    "    new_files.append((current_group, current_group_size))\n",
    "\n",
    "# Exibir os novos grupos de arquivos\n",
    "for i, (group, group_size) in enumerate(new_files, start=1):\n",
    "    logging.info(f\"Novo arquivo {i}: {group}, Tamanho: {group_size:.2f} GB\")\n",
    "\n",
    "# Combinar os arquivos de cada grupo e salvar em .parquet\n",
    "for i, (group, _) in enumerate(new_files, start=1):\n",
    "    logging.info(client)\n",
    "\n",
    "    # Ler todos os arquivos do grupo com Dask\n",
    "    table = pq.read_table([os.path.join(raw_dataset_path, file) for file in group])\n",
    "    logging.info(\"Partições lidas com sucesso.\")\n",
    "\n",
    "    # Caminho para salvar o novo arquivo\n",
    "    output_file = os.path.join(output_path, f'combined_file_{i}.parquet')\n",
    "\n",
    "    # Salvar o novo arquivo combinado\n",
    "    pq.write_table(table, output_file)\n",
    "    logging.info(f\"Arquivo combinado salvo: {output_file}\")\n",
    "    del table\n",
    "    gc.collect()\n",
    "\n",
    "    # # Ler todos os arquivos do grupo com Dask\n",
    "    # df = dd.read_parquet([os.path.join(raw_dataset_path, file) for file in group])\n",
    "\n",
    "    # # Caminho para salvar o novo arquivo\n",
    "    # output_file = os.path.join(output_path, f'combined_file_{i}.parquet')\n",
    "\n",
    "    # # Salvar o novo arquivo combinado\n",
    "    # df.to_parquet(output_file)\n",
    "    # logging.info(f\"Novo arquivo salvo: {output_file}\")\n",
    "\n",
    "# Fechar o cliente Dask\n",
    "client.close()\n",
    "logging.info(\"Client Dask fechado.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
