# Binance Tick Data Manager

High-performance pipeline for downloading and processing cryptocurrency data into Parquet files for machine learning.

## Quick Start

### 1. Installation

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Run Pipeline

```bash
# Interactive mode (recommended)
python main.py
```

## Pipeline Steps

The pipeline runs in sequential steps:

### Step 1: Download
- Download historical ZIPs from Binance
- Automatic checksum verification
- Support for Spot and Futures (USD-M / COIN-M)
- Progress saved in `download_progress_daily.json`

### Step 2: Conversion
Two options available:
- **Legacy**: ZIP â†’ CSV â†’ Parquet (slower, uses more disk)
- **Optimized**: ZIP â†’ Parquet direct (streaming, recommended)

### Step 3: Merge/Optimization
- Groups daily Parquet files into larger files (~10GB)
- Snappy compression for better performance
- Automatic cleanup of intermediate files

### Step 4: Validation
- Missing dates verification
- Parquet file integrity validation
- Corrupted file detection

### Step 5: Features
Alternative bar generation for ML:
- **Standard Dollar Bars**: Bars by fixed dollar volume
- **Imbalance Dollar Bars**: Adaptive bars based on imbalance

## Directory Structure

```
binance-tick-data-manager/
â”œâ”€â”€ main.py                    # Main entry point
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_pipeline/         # ETL modules
â”‚   â”‚   â”œâ”€â”€ downloaders/       # Binance download
â”‚   â”‚   â”œâ”€â”€ extractors/        # CSV extraction
â”‚   â”‚   â”œâ”€â”€ converters/        # Parquet conversion
â”‚   â”‚   â”œâ”€â”€ processors/        # Merge and optimization
â”‚   â”‚   â”œâ”€â”€ validators/        # Data validation
â”‚   â”‚   â””â”€â”€ utils/             # Utilities
â”‚   â”œâ”€â”€ features/              # Feature engineering
â”‚   â”‚   â””â”€â”€ bars/              # Bar generation
â”‚   â””â”€â”€ scripts/               # Helper scripts
â”œâ”€â”€ data/                      # Data (per ticker)
â”‚   â”œâ”€â”€ btcusdt-spot/
â”‚   â”‚   â”œâ”€â”€ raw-zip-daily/            # Downloaded ZIPs
â”‚   â”‚   â”œâ”€â”€ raw-parquet-daily/        # Individual Parquets
â”‚   â”‚   â”œâ”€â”€ raw-parquet-merged-daily/ # Merged Parquets (~10GB)
â”‚   â”‚   â”œâ”€â”€ output/                   # Generated features
â”‚   â”‚   â””â”€â”€ logs/                     # Local logs
â”‚   â”œâ”€â”€ btcusdt-futures-um/           # Futures USD-M
â”‚   â””â”€â”€ logs/                         # Global logs
â”œâ”€â”€ output/                    # Features (organized by ticker)
â””â”€â”€ notebooks/                 # Exploratory analysis
```

## Implemented Features

| Feature | Status | Description |
|---------|--------|-------------|
| Download with checksum | âœ… | Automatic SHA256 verification |
| Spot support | âœ… | Spot market data |
| Futures support | âœ… | USD-M and COIN-M |
| ZIP â†’ Parquet streaming | âœ… | Optimized conversion without intermediate CSV |
| Parquet merge | âœ… | Groups into ~10GB files |
| Date validation | âœ… | Detects data gaps |
| Standard Dollar Bars | âœ… | Bars by fixed dollar volume |
| Imbalance Dollar Bars | âœ… | Adaptive bars by imbalance |
| Progress tracking | âœ… | Resumes interrupted downloads |

## In Development

| Feature | Status | Description |
|---------|--------|-------------|
| Imbalance Bars (tick) | ðŸ”„ | Bars by tick imbalance |
| Unit tests | â¬œ | Automated test suite |
| Tick Bars | â¬œ | Bars by tick count |
| Volume Bars | â¬œ | Bars by contract volume |
| CLI arguments | â¬œ | Command-line execution |
| ML models | â¬œ | ML framework integration |

## Data Usage

### Reading Parquet

```python
import pandas as pd

# Single file
df = pd.read_parquet("data/btcusdt-spot/raw-parquet-merged-daily/merged_part_0.parquet")

# Multiple files with Dask (files larger than RAM)
import dask.dataframe as dd
df = dd.read_parquet("data/btcusdt-spot/raw-parquet-merged-daily/*.parquet")
```

### Reading Dollar Bars

```python
import pandas as pd

# Standard Dollar Bars
df = pd.read_parquet("data/btcusdt-spot/output/standard_dollar_bars.parquet")

# Imbalance Dollar Bars
df = pd.read_parquet("output/btcusdt-spot/imbalance_dollar_bars.parquet")
```

## Requirements

- Python 3.8+
- ~10GB disk space per year of data
- Internet connection for Binance download

## Architecture

### Design Principles

- **Simple**: No Docker, databases, or complex infrastructure
- **Fast**: Parquet files for 10x better performance than CSV
- **Reliable**: Checksum verification and data validation
- **Resumable**: Progress tracking for interrupted operations
- **Organized**: Each ticker in its own directory

### Technologies

- **PyArrow**: Parquet read/write
- **Dask**: Processing files larger than RAM
- **Pandas**: DataFrame manipulation
- **httpx**: Async HTTP downloads

## License

MIT
