# Binance Tick Data Manager

Pipeline de alta performance para download e processamento de dados de criptomoedas em arquivos Parquet para machine learning.

## Quick Start

### 1. InstalaÃ§Ã£o

```bash
# Criar ambiente virtual
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Instalar dependÃªncias
pip install -r requirements.txt
```

### 2. Executar Pipeline

```bash
# Modo interativo (recomendado)
python main.py
```

## Pipeline - Etapas

O pipeline Ã© executado em etapas sequenciais:

### Etapa 1: Download
- Download de ZIPs histÃ³ricos da Binance
- VerificaÃ§Ã£o de checksum automÃ¡tica
- Suporte a Spot e Futures (USD-M / COIN-M)
- Progresso salvo em `download_progress_daily.json`

### Etapa 2: ConversÃ£o
Duas opÃ§Ãµes disponÃ­veis:
- **Legacy**: ZIP â†’ CSV â†’ Parquet (mais lento, usa mais disco)
- **Otimizado**: ZIP â†’ Parquet direto (streaming, recomendado)

### Etapa 3: Merge/OtimizaÃ§Ã£o
- Agrupa arquivos Parquet diÃ¡rios em arquivos maiores (~10GB)
- CompressÃ£o Snappy para melhor performance
- Limpeza automÃ¡tica dos arquivos intermediÃ¡rios

### Etapa 4: ValidaÃ§Ã£o
- VerificaÃ§Ã£o de datas faltantes
- ValidaÃ§Ã£o de integridade dos arquivos Parquet
- DetecÃ§Ã£o de arquivos corrompidos

### Etapa 5: Features
GeraÃ§Ã£o de barras alternativas para ML:
- **Standard Dollar Bars**: Barras por volume em dÃ³lares fixo
- **Imbalance Dollar Bars**: Barras adaptativas baseadas em desequilÃ­brio

## Estrutura de DiretÃ³rios

```
binance-tick-data-manager/
â”œâ”€â”€ main.py                    # Entry point principal
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_pipeline/         # ETL modules
â”‚   â”‚   â”œâ”€â”€ downloaders/       # Download da Binance
â”‚   â”‚   â”œâ”€â”€ extractors/        # ExtraÃ§Ã£o CSV
â”‚   â”‚   â”œâ”€â”€ converters/        # ConversÃ£o para Parquet
â”‚   â”‚   â”œâ”€â”€ processors/        # Merge e otimizaÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ validators/        # ValidaÃ§Ã£o de dados
â”‚   â”‚   â””â”€â”€ utils/             # UtilitÃ¡rios
â”‚   â”œâ”€â”€ features/              # Feature engineering
â”‚   â”‚   â””â”€â”€ bars/              # GeraÃ§Ã£o de barras
â”‚   â””â”€â”€ scripts/               # Scripts auxiliares
â”œâ”€â”€ data/                      # Dados (por ticker)
â”‚   â”œâ”€â”€ btcusdt-spot/
â”‚   â”‚   â”œâ”€â”€ raw-zip-daily/            # ZIPs baixados
â”‚   â”‚   â”œâ”€â”€ raw-parquet-daily/        # Parquets individuais
â”‚   â”‚   â”œâ”€â”€ raw-parquet-merged-daily/ # Parquets merged (~10GB)
â”‚   â”‚   â”œâ”€â”€ output/                   # Features geradas
â”‚   â”‚   â””â”€â”€ logs/                     # Logs locais
â”‚   â”œâ”€â”€ btcusdt-futures-um/           # Futures USD-M
â”‚   â””â”€â”€ logs/                         # Logs globais
â”œâ”€â”€ output/                    # Features (organizado por ticker)
â””â”€â”€ notebooks/                 # AnÃ¡lises exploratÃ³rias
```

## Funcionalidades Implementadas

| Feature | Status | DescriÃ§Ã£o |
|---------|--------|-----------|
| Download com checksum | âœ… | VerificaÃ§Ã£o SHA256 automÃ¡tica |
| Suporte Spot | âœ… | Dados de mercado spot |
| Suporte Futures | âœ… | USD-M e COIN-M |
| ZIP â†’ Parquet streaming | âœ… | ConversÃ£o otimizada sem CSV intermediÃ¡rio |
| Merge de Parquets | âœ… | Agrupa em arquivos ~10GB |
| ValidaÃ§Ã£o de datas | âœ… | Detecta gaps nos dados |
| Standard Dollar Bars | âœ… | Barras por volume fixo em dÃ³lares |
| Imbalance Dollar Bars | âœ… | Barras adaptativas por desequilÃ­brio |
| Progress tracking | âœ… | Retoma downloads interrompidos |

## Em Desenvolvimento

| Feature | Status | DescriÃ§Ã£o |
|---------|--------|-----------|
| Imbalance Bars (tick) | ðŸ”„ | Barras por desequilÃ­brio de ticks |
| Testes unitÃ¡rios | â¬œ | Suite de testes automatizados |
| Tick Bars | â¬œ | Barras por nÃºmero de ticks |
| Volume Bars | â¬œ | Barras por volume de contratos |
| CLI arguments | â¬œ | ExecuÃ§Ã£o por linha de comando |
| Modelos ML | â¬œ | IntegraÃ§Ã£o com frameworks de ML |

## Uso dos Dados

### Leitura de Parquet

```python
import pandas as pd

# Arquivo Ãºnico
df = pd.read_parquet("data/btcusdt-spot/raw-parquet-merged-daily/merged_part_0.parquet")

# MÃºltiplos arquivos com Dask (arquivos maiores que RAM)
import dask.dataframe as dd
df = dd.read_parquet("data/btcusdt-spot/raw-parquet-merged-daily/*.parquet")
```

### Leitura de Dollar Bars

```python
import pandas as pd

# Standard Dollar Bars
df = pd.read_parquet("data/btcusdt-spot/output/standard_dollar_bars.parquet")

# Imbalance Dollar Bars
df = pd.read_parquet("output/btcusdt-spot/imbalance_dollar_bars.parquet")
```

## Requisitos

- Python 3.8+
- ~10GB espaÃ§o em disco por ano de dados
- ConexÃ£o com internet para download da Binance

## Arquitetura

### PrincÃ­pios de Design

- **Simples**: Sem Docker, databases ou infraestrutura complexa
- **RÃ¡pido**: Parquet files para 10x melhor performance que CSV
- **ConfiÃ¡vel**: VerificaÃ§Ã£o de checksum e validaÃ§Ã£o de dados
- **ResumÃ­vel**: Tracking de progresso para operaÃ§Ãµes interrompidas
- **Organizado**: Cada ticker em seu prÃ³prio diretÃ³rio

### Tecnologias

- **PyArrow**: Leitura/escrita de Parquet
- **Dask**: Processamento de arquivos maiores que RAM
- **Pandas**: ManipulaÃ§Ã£o de DataFrames
- **httpx**: Downloads HTTP async

## License

MIT
