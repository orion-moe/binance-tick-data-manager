# Bitcoin ML Finance Pipeline

Sistema avanÃ§ado de processamento e anÃ¡lise de dados de trading de Bitcoin com pipeline completo para machine learning.

## ğŸš€ Features

- **Pipeline Completo**: Download â†’ ExtraÃ§Ã£o â†’ ConversÃ£o â†’ OtimizaÃ§Ã£o â†’ ValidaÃ§Ã£o â†’ Features
- **Download Automatizado**: Dados spot e futures da Binance com verificaÃ§Ã£o de integridade
- **Processamento Otimizado**: ConversÃ£o para Parquet com tipos otimizados
- **ValidaÃ§Ã£o de Integridade**: VerificaÃ§Ã£o completa com relatÃ³rios detalhados
- **Engenharia de Features**: Imbalance dollar bars e indicadores avanÃ§ados
- **Performance**: Processamento distribuÃ­do com Dask e otimizaÃ§Ã£o com Numba

## ğŸ“‹ Estrutura do Projeto

```
degen-ml-finance/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_pipeline/
â”‚   â”‚   â”œâ”€â”€ downloaders/      # Download de dados da Binance
â”‚   â”‚   â”œâ”€â”€ extractors/       # ExtraÃ§Ã£o de arquivos CSV
â”‚   â”‚   â”œâ”€â”€ converters/       # ConversÃ£o CSV â†’ Parquet
â”‚   â”‚   â”œâ”€â”€ processors/       # OtimizaÃ§Ã£o e processamento
â”‚   â”‚   â””â”€â”€ validators/       # ValidaÃ§Ã£o de integridade
â”‚   â”œâ”€â”€ features/             # Engenharia de features
â”‚   â”œâ”€â”€ notebooks/            # AnÃ¡lises em Jupyter
â”‚   â””â”€â”€ utils/                # UtilitÃ¡rios
â”œâ”€â”€ datasets/                 # Dados baixados
â”‚   â”œâ”€â”€ dataset-raw-*/        # Arquivos ZIP/CSV
â”‚   â””â”€â”€ dataset-raw-*-compressed/  # Arquivos Parquet
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ optimized/            # Parquet otimizados
â”‚   â””â”€â”€ optimized-filled/     # Com dias faltantes preenchidos
â”œâ”€â”€ main.py                   # Entry point principal
â””â”€â”€ requirements.txt          # DependÃªncias
```

## ğŸ› ï¸ InstalaÃ§Ã£o

```bash
# Clone o repositÃ³rio
git clone https://github.com/seu-usuario/degen-ml-finance.git
cd degen-ml-finance

# Crie um ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Instale as dependÃªncias
pip install -r requirements.txt
```

## ğŸ¯ Uso RÃ¡pido

### Modo Interativo (Recomendado)

```bash
python main.py
```

O modo interativo guia vocÃª atravÃ©s de todo o pipeline:

1. **SeleÃ§Ã£o de Mercado**: Escolha sÃ­mbolo, tipo (spot/futures) e granularidade
2. **Pipeline Sequencial**: Execute cada etapa com indicadores de status
3. **VerificaÃ§Ã£o AutomÃ¡tica**: ValidaÃ§Ã£o em cada passo

### Modo Linha de Comando

```bash
# Download de dados
python main.py download --start 2024-01-01 --end 2024-01-31

# Otimizar arquivos parquet
python main.py optimize --source datasets/raw --target data/optimized

# Validar dados
python main.py validate --quick

# Gerar features
python main.py features --type imbalance
```

## ğŸ“Š Pipeline Detalhado

### 1. Download de Dados (âœ… Hash Verification)

```python
# Download com verificaÃ§Ã£o de checksum
python src/data_pipeline/downloaders/binance_downloader.py
```

- Download paralelo com mÃºltiplos workers
- VerificaÃ§Ã£o SHA256 de todos os arquivos
- Resume de downloads interrompidos
- Suporte para spot e futures (USD-M e COIN-M)

### 2. ExtraÃ§Ã£o de CSV (âœ… Integrity Check)

```python
# Extrair e verificar arquivos CSV
python src/data_pipeline/extractors/csv_extractor.py
```

- ExtraÃ§Ã£o segura de arquivos ZIP
- VerificaÃ§Ã£o de integridade dos CSV
- Limpeza opcional de ZIPs apÃ³s extraÃ§Ã£o
- Retry automÃ¡tico para falhas

### 3. ConversÃ£o para Parquet (âœ… Type Optimization)

```python
# Converter preservando nomenclatura mensal
python src/data_pipeline/converters/csv_to_parquet.py
```

- Preserva nomenclatura baseada em meses
- OtimizaÃ§Ã£o de tipos (float32 para preÃ§os)
- CompressÃ£o Snappy para eficiÃªncia
- Suporte para CSVs com/sem headers

### 4. OtimizaÃ§Ã£o de Arquivos

```python
# Combinar em arquivos de 10GB
python src/data_pipeline/processors/parquet_optimizer.py
```

- Combina arquivos pequenos em chunks maiores
- MantÃ©m ordem cronolÃ³gica
- OtimizaÃ§Ã£o com Numba JIT
- Reduz nÃºmero de arquivos para melhor I/O

### 5. Preenchimento de Dias Faltantes

```python
# Identificar e preencher gaps
python src/data_pipeline/processors/missing_days_filler.py
```

- Detecta dias de trading faltantes
- Exclui fins de semana e feriados
- Preenche com dados placeholder
- Garante sÃ©rie temporal contÃ­nua

### 6. ValidaÃ§Ã£o de Dados

```python
# ValidaÃ§Ã£o rÃ¡pida
python src/data_pipeline/validators/quick_validator.py

# ValidaÃ§Ã£o avanÃ§ada com relatÃ³rios
python src/data_pipeline/validators/advanced_validator.py
```

- VerificaÃ§Ã£o de integridade de arquivos
- AnÃ¡lise de qualidade de dados
- RelatÃ³rios detalhados em HTML/JSON
- DetecÃ§Ã£o de anomalias

### 7. GeraÃ§Ã£o de Features

```python
# Gerar imbalance dollar bars
python src/features/imbalance_bars.py
```

- Imbalance bars baseadas em volume dollar
- CÃ¡lculo de direÃ§Ã£o de mudanÃ§a de preÃ§o
- Features de microestrutura de mercado
- Processamento distribuÃ­do com Dask

## ğŸ“ˆ Exemplos de CÃ³digo

### Download Completo para 2024

```python
from datetime import datetime
from src.data_pipeline.downloaders.binance_downloader import BinanceDataDownloader

# Configurar downloader
downloader = BinanceDataDownloader(
    symbol="BTCUSDT",
    data_type="spot",
    granularity="monthly"
)

# Download de todo 2024
start = datetime(2024, 1, 1)
end = datetime(2024, 12, 31)
downloader.download_date_range(start, end, max_workers=10)
```

### Pipeline Completo Automatizado

```python
# Pipeline completo para um sÃ­mbolo
def run_complete_pipeline(symbol="BTCUSDT", year=2024):
    # 1. Download
    downloader = BinanceDataDownloader(symbol=symbol)
    downloader.download_date_range(
        datetime(year, 1, 1), 
        datetime(year, 12, 31)
    )
    
    # 2. Extrair CSV
    extractor = CSVExtractor(symbol=symbol)
    extractor.extract_and_verify_all()
    
    # 3. Converter para Parquet
    converter = CSVToParquetConverter(symbol=symbol)
    converter.convert_all_csv_files(cleanup_csv=True)
    
    # 4. Otimizar
    # ... (executar otimizador)
    
    # 5. Preencher dias faltantes
    filler = MissingDaysFiller(symbol=symbol)
    filler.fill_all_missing_days()
    
    # 6. Validar
    # ... (executar validadores)
    
    # 7. Gerar features
    # ... (executar gerador de features)
```

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

```bash
# Base directory para dados (opcional)
export DEGEN_ML_BASE_DIR=/path/to/data

# NÃºmero de workers para download (opcional)
export DEGEN_ML_WORKERS=10

# NÃ­vel de log (opcional)
export DEGEN_ML_LOG_LEVEL=INFO
```

### Arquivos de Progresso

O pipeline mantÃ©m arquivos de progresso para retomar operaÃ§Ãµes:

- `download_progress_*.json` - Arquivos baixados
- `extraction_progress_*.json` - Arquivos extraÃ­dos
- `conversion_progress_*.json` - Arquivos convertidos

## ğŸ§ª Testes

```bash
# Executar todos os testes
python -m pytest tests/

# Testes com coverage
python -m pytest --cov=src tests/

# Testes especÃ­ficos
python -m pytest tests/test_downloader.py
```

## ğŸ› Troubleshooting

### Problemas Comuns

1. **Falhas de Download**
   - Verifique conexÃ£o com internet
   - Binance pode ter limites de rate
   - Use menos workers se necessÃ¡rio

2. **Erros de MemÃ³ria**
   - Reduza chunk_size no processamento
   - Use Dask para processamento distribuÃ­do
   - Processe em batches menores

3. **EspaÃ§o em Disco**
   - Cada mÃªs de dados ~5-10GB
   - Use compressÃ£o Parquet
   - Limpe arquivos intermediÃ¡rios

4. **Dados Faltantes**
   - Fins de semana nÃ£o tÃªm trading 24/7
   - Alguns feriados podem ter gaps
   - Use o preenchedor de dias faltantes

### Logs Detalhados

Verifique logs em:
- `datasets/logs/` - Logs do pipeline
- `reports/` - RelatÃ³rios de validaÃ§Ã£o

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

## ğŸ™ Agradecimentos

- Binance por disponibilizar dados histÃ³ricos
- Comunidade Python por ferramentas incrÃ­veis
- Dask e Numba por processamento de alta performance
- PyArrow/Parquet por armazenamento eficiente